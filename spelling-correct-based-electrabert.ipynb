{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9016169,"sourceType":"datasetVersion","datasetId":5432781}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport time\nimport random\nimport os","metadata":{"id":"mGYu1I2i4AuN","execution":{"iopub.status.busy":"2024-07-24T02:10:47.047084Z","iopub.execute_input":"2024-07-24T02:10:47.047430Z","iopub.status.idle":"2024-07-24T02:10:53.266068Z","shell.execute_reply.started":"2024-07-24T02:10:47.047380Z","shell.execute_reply":"2024-07-24T02:10:53.265122Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Ultis","metadata":{"id":"fg2u5KBoxGIv"}},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\n\nspecial_char = {\n    \"x\": \"xs\",\n    \"s\": \"sx\",\n    \"d\": \"drg\",\n    \"r\": \"rdg\",\n    \"g\": \"grd\",\n    \"ch\": \"tr\",\n    \"tr\": \"ch\",\n    \"n\": \"l\",\n    \"l\": \"n\",\n}\n\nvietnamese_characters = [\n    \"á\",\n    \"à\",\n    \"ả\",\n    \"ã\",\n    \"ạ\",\n    \"ă\",\n    \"ắ\",\n    \"ằ\",\n    \"ẳ\",\n    \"ẵ\",\n    \"ặ\",\n    \"â\",\n    \"ấ\",\n    \"ầ\",\n    \"ẩ\",\n    \"ẫ\",\n    \"ậ\",\n    \"é\",\n    \"è\",\n    \"ẻ\",\n    \"ẽ\",\n    \"ẹ\",\n    \"ê\",\n    \"ế\",\n    \"ề\",\n    \"ể\",\n    \"ễ\",\n    \"ệ\",\n    \"í\",\n    \"ì\",\n    \"ỉ\",\n    \"ĩ\",\n    \"ị\",\n    \"ó\",\n    \"ò\",\n    \"ỏ\",\n    \"õ\",\n    \"ọ\",\n    \"ô\",\n    \"ố\",\n    \"ồ\",\n    \"ổ\",\n    \"ỗ\",\n    \"ộ\",\n    \"ơ\",\n    \"ớ\",\n    \"ờ\",\n    \"ở\",\n    \"ỡ\",\n    \"ợ\",\n    \"ú\",\n    \"ù\",\n    \"ủ\",\n    \"ũ\",\n    \"ụ\",\n    \"ư\",\n    \"ứ\",\n    \"ừ\",\n    \"ử\",\n    \"ữ\",\n    \"ự\",\n    \"ý\",\n    \"ỳ\",\n    \"ỷ\",\n    \"ỹ\",\n    \"ỵ\",\n    \"đ\",\n]\n# sac, huyen, hoi, nga, nang, mu, rau, mu nguoc :>\nspecial_token_sign_vietnamese = {\n    \"sac\": \"<´>\",\n    \"huyen\": \"<`>\",\n    \"hoi\": \"<?>\",\n    \"nga\": \"<~>\",\n    \"nang\": \"<.>\",\n    \"mu\": \"<^>\",\n    \"rau\": \"<,>\",\n    \"mu_nguoc\": \"<^^>\",\n}\nconvert_char_vietnamese = {\n    \"á\": (\"a\", special_token_sign_vietnamese[\"sac\"]),\n    \"à\": (\"a\", special_token_sign_vietnamese[\"huyen\"]),\n    \"ả\": (\"a\", special_token_sign_vietnamese[\"hoi\"]),\n    \"ã\": (\"a\", special_token_sign_vietnamese[\"nga\"]),\n    \"ạ\": (\"a\", special_token_sign_vietnamese[\"nang\"]),\n    \"ă\": (\"a\", special_token_sign_vietnamese[\"mu_nguoc\"]),\n    \"ắ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu_nguoc\"],\n        special_token_sign_vietnamese[\"sac\"],\n    ),\n    \"ằ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu_nguoc\"],\n        special_token_sign_vietnamese[\"huyen\"],\n    ),\n    \"ẳ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu_nguoc\"],\n        special_token_sign_vietnamese[\"hoi\"],\n    ),\n    \"ẵ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu_nguoc\"],\n        special_token_sign_vietnamese[\"nga\"],\n    ),\n    \"ặ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu_nguoc\"],\n        special_token_sign_vietnamese[\"nang\"],\n    ),\n    \"â\": (\"a\", special_token_sign_vietnamese[\"mu\"]),\n    \"ấ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"sac\"],\n    ),\n    \"ầ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"huyen\"],\n    ),\n    \"ẩ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"hoi\"],\n    ),\n    \"ẫ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"nga\"],\n    ),\n    \"ậ\": (\n        \"a\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"nang\"],\n    ),\n    \"é\": (\"e\", special_token_sign_vietnamese[\"sac\"]),\n    \"è\": (\"e\", special_token_sign_vietnamese[\"huyen\"]),\n    \"ẻ\": (\"e\", special_token_sign_vietnamese[\"hoi\"]),\n    \"ẽ\": (\"e\", special_token_sign_vietnamese[\"nga\"]),\n    \"ẹ\": (\"e\", special_token_sign_vietnamese[\"nang\"]),\n    \"ê\": (\"e\", special_token_sign_vietnamese[\"mu\"]),\n    \"ế\": (\n        \"e\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"sac\"],\n    ),\n    \"ề\": (\n        \"e\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"huyen\"],\n    ),\n    \"ể\": (\n        \"e\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"hoi\"],\n    ),\n    \"ễ\": (\n        \"e\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"nga\"],\n    ),\n    \"ệ\": (\n        \"e\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"nang\"],\n    ),\n    \"í\": (\"i\", special_token_sign_vietnamese[\"sac\"]),\n    \"ì\": (\"i\", special_token_sign_vietnamese[\"huyen\"]),\n    \"ỉ\": (\"i\", special_token_sign_vietnamese[\"hoi\"]),\n    \"ĩ\": (\"i\", special_token_sign_vietnamese[\"nga\"]),\n    \"ị\": (\"i\", special_token_sign_vietnamese[\"nang\"]),\n    \"ó\": (\"o\", special_token_sign_vietnamese[\"sac\"]),\n    \"ò\": (\"o\", special_token_sign_vietnamese[\"huyen\"]),\n    \"ỏ\": (\"o\", special_token_sign_vietnamese[\"hoi\"]),\n    \"õ\": (\"o\", special_token_sign_vietnamese[\"nga\"]),\n    \"ọ\": (\"o\", special_token_sign_vietnamese[\"nang\"]),\n    \"ô\": (\"o\", special_token_sign_vietnamese[\"mu\"]),\n    \"ố\": (\n        \"o\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"sac\"],\n    ),\n    \"ồ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"huyen\"],\n    ),\n    \"ổ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"hoi\"],\n    ),\n    \"ỗ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"nga\"],\n    ),\n    \"ộ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"mu\"],\n        special_token_sign_vietnamese[\"nang\"],\n    ),\n    \"ơ\": (\"o\", special_token_sign_vietnamese[\"rau\"]),\n    \"ớ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"sac\"],\n    ),\n    \"ờ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"huyen\"],\n    ),\n    \"ở\": (\n        \"o\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"hoi\"],\n    ),\n    \"ỡ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"nga\"],\n    ),\n    \"ợ\": (\n        \"o\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"nang\"],\n    ),\n    \"ú\": (\"u\", special_token_sign_vietnamese[\"sac\"]),\n    \"ù\": (\"u\", special_token_sign_vietnamese[\"huyen\"]),\n    \"ủ\": (\"u\", special_token_sign_vietnamese[\"hoi\"]),\n    \"ũ\": (\"u\", special_token_sign_vietnamese[\"nga\"]),\n    \"ụ\": (\"u\", special_token_sign_vietnamese[\"nang\"]),\n    \"ư\": (\"u\", special_token_sign_vietnamese[\"rau\"]),\n    \"ứ\": (\n        \"u\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"sac\"],\n    ),\n    \"ừ\": (\n        \"u\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"huyen\"],\n    ),\n    \"ử\": (\n        \"u\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"hoi\"],\n    ),\n    \"ữ\": (\n        \"u\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"nga\"],\n    ),\n    \"ự\": (\n        \"u\",\n        special_token_sign_vietnamese[\"rau\"],\n        special_token_sign_vietnamese[\"nang\"],\n    ),\n    \"ý\": (\"y\", special_token_sign_vietnamese[\"sac\"]),\n    \"ỳ\": (\"y\", special_token_sign_vietnamese[\"huyen\"]),\n    \"ỷ\": (\"y\", special_token_sign_vietnamese[\"hoi\"]),\n    \"ỹ\": (\"y\", special_token_sign_vietnamese[\"nga\"]),\n    \"ỵ\": (\"y\", special_token_sign_vietnamese[\"nang\"]),\n    \"đ\": (\"d\", \"d\"),\n}\n\n\ndef write_dict_data_to_file(file_path, data, indent=2):\n    with open(file_path, \"w\", encoding=\"utf-8\") as outfile:\n        json.dump(data, outfile, ensure_ascii=False, indent=indent)\n\n\ndef write_list_data_to_file(file_path, list_data):\n    # os.makedirs(file_path, exist_ok=True)\n    with open(file=file_path, mode=\"w\", encoding=\"utf-8\") as file:\n        file.write(\"\".join([x + \"\\n\" for x in list_data]))\n","metadata":{"id":"zlt93RuIxB5t","execution":{"iopub.status.busy":"2024-07-24T02:10:53.268383Z","iopub.execute_input":"2024-07-24T02:10:53.269087Z","iopub.status.idle":"2024-07-24T02:10:53.693540Z","shell.execute_reply.started":"2024-07-24T02:10:53.269049Z","shell.execute_reply":"2024-07-24T02:10:53.692780Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport string\n\n\ndef load_data_and_vocab(\n    path_file_data=None,\n    data_size=1000,\n):\n    file = open(path_file_data, \"r\")\n    sentences = file.readlines()\n    sentences = [\n        replace_punctuation(sentence.replace(\"\\n\", \"\").lower())\n        for sentence in sentences\n    ]\n    return sentences[:data_size]\n\n\ndef replace_punctuation(text, replacement=\"\"):\n    translator = str.maketrans({key: replacement for key in string.punctuation})\n    return text.translate(translator)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:10:53.694473Z","iopub.execute_input":"2024-07-24T02:10:53.694867Z","iopub.status.idle":"2024-07-24T02:10:53.701051Z","shell.execute_reply.started":"2024-07-24T02:10:53.694842Z","shell.execute_reply":"2024-07-24T02:10:53.700213Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import collections\nimport re\nimport unicodedata\nimport six\n\n\nclass CreateVocab(object):\n    def __init__(self, special_token):\n        self.special_token = special_token\n\n    def create_word_vocab(self, data_text, vocab_size=None):\n        print(\"Creating vocab:\")\n        word_vocab = collections.OrderedDict()\n        list_word_token = collections.OrderedDict()\n        for sentence in data_text:\n            list_token = convert_sentence_to_token(sentence)\n            for token in list_token:\n                if contains_number(token) or contains_special_character(token):\n                    continue\n                if token in list_word_token:\n                    list_word_token[token] = list_word_token[token] + 1\n                else:\n                    list_word_token[token] = 1\n        list_word_token = collections.OrderedDict(\n            sorted(list_word_token.items(), key=lambda x: x[1], reverse=True)\n        )\n        index = 0\n        for token in self.special_token:\n            word_vocab[token] = index\n            index += 1\n        for token in list_word_token:\n            word_vocab[token] = index\n            if vocab_size != None:\n                if index > vocab_size - len(self.special_token):\n                    break\n            index += 1\n        return word_vocab\n\n    def create_character_vocab(self, file_text, vocab_size=None):\n        character_vocab = collections.OrderedDict()\n        list_char_token = collections.OrderedDict()\n        for senetence in file_text:\n            for char in senetence:\n                if contains_special_character(token=char):\n                    continue\n                if char in list_char_token:\n                    list_char_token[char] = list_char_token[char] + 1\n                else:\n                    list_char_token[char] = 1\n        list_char_token = collections.OrderedDict(\n            sorted(list_char_token.items(), key=lambda x: x[1], reverse=True)\n        )\n        index = 0\n        for token in self.special_token:\n            character_vocab[token] = index\n            index += 1\n        for token in list_char_token:\n            if token in vietnamese_characters:\n                continue\n            character_vocab[token] = index\n            if vocab_size != None:\n                if index > vocab_size - len(self.special_token):\n                    break\n            index += 1\n        for token in special_token_sign_vietnamese.values():\n            character_vocab[token] = index\n            index += 1\n        return character_vocab\n\n\ndef contains_special_character(token):\n    for char in token:\n        if _is_whitespace(char=char):\n            return True\n        # if _is_punctuation(char=char):\n        #     return True\n    return False\n\n\ndef contains_number(text):\n    pattern = re.compile(r\"[0-9]\")\n    return bool(pattern.search(text))\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef convert_sentence_to_token(text):\n    \"\"\"Tokenizes a piece of text.\"\"\"\n    text = convert_to_unicode(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        # if self.do_lower_case:\n        token = token.lower()\n        # token = self._run_strip_accents(token)\n        split_tokens.extend(run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n\n\ndef run_split_on_punc(text):\n    \"\"\"Splits punctuation on a piece of text.\"\"\"\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n        char = chars[i]\n        if _is_punctuation(char):\n            output.append([char])\n            start_new_word = True\n        else:\n            if start_new_word:\n                output.append([])\n            start_new_word = False\n            output[-1].append(char)\n        i += 1\n\n    return [\"\".join(x) for x in output]\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if (\n        (cp >= 33 and cp <= 47)\n        or (cp >= 58 and cp <= 64)\n        or (cp >= 91 and cp <= 96)\n        or (cp >= 123 and cp <= 126)\n    ):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n","metadata":{"id":"cdNJUx01xOaG","execution":{"iopub.status.busy":"2024-07-24T02:10:53.703403Z","iopub.execute_input":"2024-07-24T02:10:53.703693Z","iopub.status.idle":"2024-07-24T02:10:53.730694Z","shell.execute_reply.started":"2024-07-24T02:10:53.703671Z","shell.execute_reply":"2024-07-24T02:10:53.729915Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nimport torch\n\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, char_input_ids, word_input_ids, mask_token_positions, target):\n        self.char_input_ids = char_input_ids\n        self.word_input_ids = word_input_ids\n        self.mask_token_positions = mask_token_positions\n        self.target = target\n\n    def __getitem__(self, idx):\n        target = self.target[idx]\n        word_input_ids = self.word_input_ids[idx]\n        mask_token_positions = self.mask_token_positions[idx]\n        char_input_ids = self.char_input_ids[idx]\n\n        return {\n            \"word_input_ids\": word_input_ids,\n            \"mask_token_positions\": mask_token_positions,\n            \"char_input_ids\": char_input_ids,\n        }, target\n\n    def __len__(self):\n        return len(self.target)\n\n    def collate_fn(self, batch):\n        word_input_ids = [item[0][\"word_input_ids\"] for item in batch]\n        mask_token_positions = [item[0][\"mask_token_positions\"] for item in batch]\n        char_input_ids = [item[0][\"char_input_ids\"] for item in batch]\n        targets = [item[1] for item in batch]\n\n        # Pad sequences\n        word_input_ids_padded = pad_sequence(\n            word_input_ids, batch_first=True, padding_value=0\n        )\n        mask_token_positions_padded = pad_sequence(\n            mask_token_positions, batch_first=True, padding_value=0\n        )\n        char_input_ids_padded = pad_sequence(\n            char_input_ids, batch_first=True, padding_value=0\n        )\n\n        return {\n            \"word_input_ids\": word_input_ids_padded,\n            \"mask_token_positions\": mask_token_positions_padded,\n            \"char_input_ids\": char_input_ids_padded,\n        }, targets\n","metadata":{"id":"ujCH1c5KxSBi","execution":{"iopub.status.busy":"2024-07-24T02:10:53.731672Z","iopub.execute_input":"2024-07-24T02:10:53.731978Z","iopub.status.idle":"2024-07-24T02:10:53.745377Z","shell.execute_reply.started":"2024-07-24T02:10:53.731948Z","shell.execute_reply":"2024-07-24T02:10:53.744703Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport re\nimport collections\nimport unicodedata\nimport six\n\n\nclass Tokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(\n        self, vocab_file, unk_token=\"[UNK]\", do_lower_case=True, char_level=False\n    ):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.do_lower_case = do_lower_case\n        self.char_level = char_level\n        self.unk_token = unk_token\n\n    def tokenize(self, text):\n        if self.char_level:\n            return self.__tokenize_char(text=text)\n        else:\n            return self.__tokenize_word(text=text)\n\n    def __tokenize_word(self, text):\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n            split_tokens.extend(self._run_split_on_punc(token))\n        return [\n            token if token in self.vocab else self.unk_token\n            for token in whitespace_tokenize(\" \".join(split_tokens))\n        ]\n\n    def __tokenize_char(self, text):\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n            split_tokens.extend(self._run_split_on_punc(token))\n        output = []\n        for token in whitespace_tokenize(\" \".join(split_tokens)):\n            char_output = []\n            for char in token:\n                if char in vietnamese_characters:\n                    for spec_token in convert_char_vietnamese[char]:\n                        char_output.append(spec_token)\n                    continue\n                char_output.append(char if char in self.vocab else self.unk_token)\n            output.append(char_output)\n        return output\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xFFFD or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(file=vocab_file, mode=\"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef contains_special_character(token):\n    for char in token:\n        if _is_whitespace(char=char):\n            return True\n        if _is_punctuation(char=char):\n            return True\n    return False\n\n\ndef contains_number(text):\n    pattern = re.compile(r\"[0-9]\")\n    return bool(pattern.search(text))\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef convert_sentence_to_token(text):\n    \"\"\"Tokenizes a piece of text.\"\"\"\n    text = convert_to_unicode(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        # if self.do_lower_case:\n        token = token.lower()\n        # token = self._run_strip_accents(token)\n        split_tokens.extend(run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        # elif isinstance(text, unicode):\n        #   return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef run_split_on_punc(text):\n    \"\"\"Splits punctuation on a piece of text.\"\"\"\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n        char = chars[i]\n        if _is_punctuation(char):\n            output.append([char])\n            start_new_word = True\n        else:\n            if start_new_word:\n                output.append([])\n            start_new_word = False\n            output[-1].append(char)\n        i += 1\n\n    return [\"\".join(x) for x in output]\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if (\n        (cp >= 33 and cp <= 47)\n        or (cp >= 58 and cp <= 64)\n        or (cp >= 91 and cp <= 96)\n        or (cp >= 123 and cp <= 126)\n    ):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n","metadata":{"id":"JkDFs-uCxVxp","execution":{"iopub.status.busy":"2024-07-24T02:10:53.746663Z","iopub.execute_input":"2024-07-24T02:10:53.746929Z","iopub.status.idle":"2024-07-24T02:10:53.780935Z","shell.execute_reply.started":"2024-07-24T02:10:53.746897Z","shell.execute_reply":"2024-07-24T02:10:53.780157Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import collections\nimport unicodedata\nfrom transformers import AutoTokenizer\nimport torch\n\n\nclass DataBuilderForElectra:\n    def __init__(\n        self,\n        rng,\n        seq_max_length,\n        tokenizer,\n    ):\n        self.rng = rng\n        self.tokenizer: AutoTokenizer = tokenizer\n        self.seq_max_length = seq_max_length\n\n    def build_data(self, inputs):\n        input_original = []\n        input_masked = []\n        for sentence in inputs:\n            tokens = split_token(sentence)\n            tokens.insert(0, self.tokenizer.cls_token)\n            if len(tokens) > self.seq_max_length:\n                tokens = tokens[: self.seq_max_length]\n            tokens_masked = create_masked_lm_predictions(\n                tokens=tokens,\n                masked_lm_prob=0.20,\n                max_predictions_per_seq=3,\n                rng=self.rng,\n            )\n            tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n            tokens_masked = self.tokenizer.convert_tokens_to_ids(tokens_masked)\n            input_original.append(\n                padding(\n                    tokens, self.seq_max_length, padding_id=self.tokenizer.pad_token_id\n                )\n            )\n            input_masked.append(\n                padding(\n                    tokens_masked,\n                    self.seq_max_length,\n                    padding_id=self.tokenizer.pad_token_id,\n                )\n            )\n        return torch.Tensor(input_original).long(), torch.Tensor(input_masked).long()\n\n\ndef split_token(text):\n    text = _clean_text(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        token = token.lower()\n        split_tokens.extend(run_split_on_punc(token))\n    return [token for token in whitespace_tokenize(\" \".join(split_tokens))]\n\n\ndef padding(input_ids, max_length, padding_id):\n    len_sentence = len(input_ids)\n    while len(input_ids) > max_length:\n        del input_ids[-1]\n    for i in range(max_length - len_sentence):\n        input_ids.append(padding_id)\n    return input_ids\n\n\nMaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\", [\"index\", \"label\"])\n\n\ndef create_masked_lm_predictions(\n    tokens,\n    masked_lm_prob,\n    max_predictions_per_seq,\n    rng,\n    cls_token=\"<cls>\",\n    sep_token=\"<sep>\",\n    mask_token=\"<mask>\",\n):\n    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n\n    cand_indexes = []\n    for i, token in enumerate(tokens):\n        if token == cls_token or token == sep_token:\n            continue\n        cand_indexes.append([i])\n\n    rng.shuffle(cand_indexes)\n\n    output_tokens = list(tokens)\n\n    num_to_predict = min(\n        max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob)))\n    )\n\n    masked_lms = []\n    covered_indexes = set()\n    for index_set in cand_indexes:\n        if len(masked_lms) >= num_to_predict:\n            break\n        # If adding a whole-word mask would exceed the maximum number of\n        # predictions, then just skip this candidate.\n        if len(masked_lms) + len(index_set) > num_to_predict:\n            continue\n        is_any_index_covered = False\n        for index in index_set:\n            if index in covered_indexes:\n                is_any_index_covered = True\n                break\n        if is_any_index_covered:\n            continue\n        for index in index_set:\n            covered_indexes.add(index)\n            masked_token = None\n            masked_token = mask_token\n            output_tokens[index] = masked_token\n            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n    assert len(masked_lms) <= num_to_predict\n    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n\n    masked_lm_positions = []\n    masked_lm_labels = []\n    for p in masked_lms:\n        masked_lm_positions.append(p.index)\n        masked_lm_labels.append(p.label)\n\n    return output_tokens\n\n\ndef _clean_text(text):\n    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n    output = []\n    for char in text:\n        cp = ord(char)\n        if cp == 0 or cp == 0xFFFD or _is_control(char):\n            continue\n        if _is_whitespace(char):\n            output.append(\" \")\n        else:\n            output.append(char)\n    return \"\".join(output)\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\ndef run_split_on_punc(text):\n    \"\"\"Splits punctuation on a piece of text.\"\"\"\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n        char = chars[i]\n        if _is_punctuation(char):\n            output.append([char])\n            start_new_word = True\n        else:\n            if start_new_word:\n                output.append([])\n            start_new_word = False\n            output[-1].append(char)\n        i += 1\n\n    return [\"\".join(x) for x in output]\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if (\n        (cp >= 33 and cp <= 47)\n        or (cp >= 58 and cp <= 64)\n        or (cp >= 91 and cp <= 96)\n        or (cp >= 123 and cp <= 126)\n    ):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False\n","metadata":{"id":"A29cEx_Xxwbd","execution":{"iopub.status.busy":"2024-07-24T02:10:53.782315Z","iopub.execute_input":"2024-07-24T02:10:53.782608Z","iopub.status.idle":"2024-07-24T02:10:55.722560Z","shell.execute_reply.started":"2024-07-24T02:10:53.782585Z","shell.execute_reply":"2024-07-24T02:10:55.721598Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import collections\nimport random\nimport torch\n# from utils import special_char\n\n\nclass DataBuilder:\n    def __init__(\n        self,\n        rng,\n        padding_id,\n        mask_token,\n        seq_max_length,\n        word_max_length,\n        custom_tokenizer: Tokenizer,\n        char_tokenizer: Tokenizer,\n    ):\n        self.rng = rng\n        self.custom_tokenizer = custom_tokenizer\n        self.char_tokenizer = char_tokenizer\n        self.word_max_length = word_max_length\n        self.seq_max_length = seq_max_length\n        self.padding_id = padding_id\n        self.mask_token = mask_token\n\n    def build_data(self, inputs):\n        word_input_ids = []\n        mask_token_positions = []\n        target = []\n        char_input_ids = []\n        for seq in inputs:\n            seq_char_ids = []\n            seq_char = seq\n            for word in seq_char.split(\" \"):\n                for start_char in special_char.keys():\n                    if word.startswith(start_char):\n                        seq_char = seq_char.replace(\n                            word, special_char[start_char] + word\n                        )\n            tokens = self.char_tokenizer.tokenize(seq_char)\n            for word in tokens:\n                word_char_ids = self.char_tokenizer.convert_tokens_to_ids(word)\n                word_char_ids = padding(\n                    word_char_ids, self.word_max_length, self.padding_id\n                )\n                seq_char_ids.append(word_char_ids)\n            char_pad = padding([], self.word_max_length, self.padding_id)\n            seq_char_ids = padding(seq_char_ids, self.seq_max_length, char_pad)\n            tokens = self.custom_tokenizer.tokenize(seq)\n            if len(tokens) > self.seq_max_length:\n                for i in range(len(tokens) - self.seq_max_length):\n                    list_tokens = tokens[i : i + self.seq_max_length]\n                    token_masked, masked_positions, masked_label = (\n                        create_masked_lm_predictions(\n                            rng=self.rng,\n                            tokens=list_tokens,\n                            vocab_words=self.custom_tokenizer.inv_vocab,\n                            mask_token=self.mask_token,\n                            masked_lm_prob=0.15,\n                            max_predictions_per_seq=5,\n                        )\n                    )\n                    word_input_ids = word_input_ids + [\n                        self.custom_tokenizer.convert_tokens_to_ids(_)\n                        for _ in token_masked\n                    ]\n                    for _ in masked_positions:\n                        __ = torch.zeros(self.seq_max_length, dtype=bool)\n                        __[_] = True\n                        mask_token_positions.append(__)\n                    target = target + [\n                        self.custom_tokenizer.convert_tokens_to_ids(_)\n                        for _ in masked_label\n                    ]\n                    char_input_ids = char_input_ids + [\n                        seq_char_ids for _ in range(len(token_masked))\n                    ]\n            else:\n                token_masked, masked_positions, masked_label = (\n                    create_masked_lm_predictions(\n                        rng=self.rng,\n                        tokens=tokens,\n                        vocab_words=self.custom_tokenizer.inv_vocab,\n                        mask_token=self.mask_token,\n                        masked_lm_prob=0.15,\n                        max_predictions_per_seq=5,\n                    )\n                )\n                word_input_ids = word_input_ids + [\n                    padding(\n                        self.custom_tokenizer.convert_tokens_to_ids(_),\n                        max_length=self.seq_max_length,\n                        padding_id=1,\n                    )\n                    for _ in token_masked\n                ]\n                for _ in masked_positions:\n                    __ = torch.zeros(self.seq_max_length, dtype=bool)\n                    __[_] = True\n                    mask_token_positions.append(__)\n                target = target + [\n                    self.custom_tokenizer.convert_tokens_to_ids(_) for _ in masked_label\n                ]\n                char_input_ids = char_input_ids + [\n                    seq_char_ids for _ in range(len(token_masked))\n                ]\n\n        return (\n            torch.tensor(char_input_ids),\n            torch.tensor(word_input_ids),\n            torch.stack(mask_token_positions),\n            target,\n        )\n\n\ndef padding(input_ids, max_length, padding_id):\n    len_sentence = len(input_ids)\n    while len(input_ids) > max_length:\n        del input_ids[-1]\n    for i in range(max_length - len_sentence):\n        input_ids.append(padding_id)\n    return input_ids\n\n\nMaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\", [\"index\", \"label\"])\n\n\ndef create_masked_char_input(\n    tokens: list, vocab_chars, mask_token, rng: random.Random, masked_lm_prob=0.25\n):\n    size_alter = int(len(tokens) * (1 + masked_lm_prob))\n    number_range = list(range(size_alter))\n    output = tokens\n    cand_indexes = rng.sample(number_range, size_alter - len(output))\n    cand_indexes.sort()\n    for index in cand_indexes:\n        random_char_token = vocab_chars[rng.randint(0, len(vocab_chars) - 1)]\n        output.insert(index, random_char_token)\n    output.append(mask_token)\n    return output\n\n\ndef create_masked_lm_predictions(\n    tokens: list,\n    masked_lm_prob,\n    max_predictions_per_seq,\n    vocab_words: dict,\n    rng,\n    cls_token=\"<cls>\",\n    sep_token=\"<sep>\",\n    mask_token=\"<mask>\",\n):\n    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n\n    cand_indexes = []\n    for i, token in enumerate(tokens):\n        if token == cls_token or token == sep_token:\n            continue\n        cand_indexes.append(i)\n\n    rng.shuffle(cand_indexes)\n\n    output_tokens = []\n\n    num_to_predict = min(\n        max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob)))\n    )\n    list_masked_lms = []\n    list_masked_lm_positions = []\n    list_masked_lm_labels = []\n    covered_indexes = set()\n    while len(covered_indexes) < len(cand_indexes):\n        masked_lms = []\n        tokens_masked = list(tokens)\n        for index in cand_indexes:\n            if index in covered_indexes:\n                continue\n            covered_indexes.add(index)\n\n            masked_token = None\n            # 80% of the time, replace with [MASK]\n            if rng.random() < 0.8:\n                masked_token = mask_token\n            else:\n                # 10% of the time, keep original\n                if rng.random() < 0.5:\n                    masked_token = tokens[index]\n                # 10% of the time, replace with random word\n                else:\n                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n\n            tokens_masked[index] = masked_token\n            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n            if len(masked_lms) >= num_to_predict:\n                masked_lms = sorted(masked_lms, key=lambda x: x.index)\n                masked_lm_positions = []\n                masked_lm_labels = []\n                for p in masked_lms:\n                    masked_lm_positions.append(p.index)\n                    masked_lm_labels.append(p.label)\n                break\n        output_tokens.append(tokens_masked)\n        list_masked_lms.append(masked_lms)\n        list_masked_lm_positions.append(masked_lm_positions)\n        list_masked_lm_labels.append(masked_lm_labels)\n\n    return (output_tokens, list_masked_lm_positions, list_masked_lm_labels)","metadata":{"id":"41f8KRSUx3QH","execution":{"iopub.status.busy":"2024-07-24T02:10:55.724233Z","iopub.execute_input":"2024-07-24T02:10:55.724599Z","iopub.status.idle":"2024-07-24T02:10:55.756034Z","shell.execute_reply.started":"2024-07-24T02:10:55.724567Z","shell.execute_reply":"2024-07-24T02:10:55.755181Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# from utils import write_list_data_to_file\nimport pandas as pd\nimport string\n\n\ndef load_data_and_vocab(\n    path_file_data=None,\n    data_size=1000,\n):\n    file = open(path_file_data, \"r\")\n    sentences = file.readlines()\n    sentences = [\n        replace_punctuation(sentence.replace(\"\\n\", \"\").lower())\n        for sentence in sentences\n    ]\n    return sentences[:data_size]\n\n\ndef replace_punctuation(text, replacement=\"\"):\n    translator = str.maketrans({key: replacement for key in string.punctuation})\n    return text.translate(translator)\n","metadata":{"id":"sjBr22HTx8TH","execution":{"iopub.status.busy":"2024-07-24T02:10:55.757255Z","iopub.execute_input":"2024-07-24T02:10:55.757590Z","iopub.status.idle":"2024-07-24T02:10:55.772428Z","shell.execute_reply.started":"2024-07-24T02:10:55.757559Z","shell.execute_reply":"2024-07-24T02:10:55.771568Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Transformer","metadata":{"id":"WaoayVoWwb2x"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef positional_encoding(length, depth):\n    depth = depth // 2\n\n    positions = np.arange(length)[:, np.newaxis]\n    depths = np.arange(depth)[np.newaxis, :] / depth\n\n    angle_rates = 1 / (10000**depths)\n    angle_rads = positions * angle_rates\n\n    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n\n    return torch.tensor(pos_encoding, dtype=torch.float32)\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model, positional=True):\n        super(PositionalEmbedding, self).__init__()\n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        self.positional = positional\n        if positional:\n            self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n\n    def forward(self, x):\n        length = x.size(1)\n        # x.size() return torch.Size([3200, 10])\n        # if torch.max(x) >= self.embedding.num_embeddings:\n        #     raise ValueError(\"Input indices are out of range for the embedding layer.\")\n        x = self.embedding(x)\n        x = x * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)).to(x.device)\n        if self.positional:\n            x = x + self.pos_encoding[:length, :].unsqueeze(0).to(x.device)\n        return x\n","metadata":{"id":"EgDg-RSXwJBl","execution":{"iopub.status.busy":"2024-07-24T02:10:55.776046Z","iopub.execute_input":"2024-07-24T02:10:55.776447Z","iopub.status.idle":"2024-07-24T02:10:55.788998Z","shell.execute_reply.started":"2024-07-24T02:10:55.776417Z","shell.execute_reply":"2024-07-24T02:10:55.788173Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(SelfAttention, self).__init__()\n        # self.mha = torch.nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,dropout=dropout)\n        self.mha = nn.MultiheadAttention(\n            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n        )\n        self.layernorm = nn.LayerNorm(normalized_shape=embed_dim)\n\n    # @torch.jit.unused(\n    def forward(self, *inputs):\n        # x (batch_size x seq_length x embed_dim), mask (batch_size . numhead x seq_length x seq_length)\n        x, key_padding_mask = inputs\n        (attn_output, attn_output_weight) = self.mha(\n            query=x, value=x, key=x, key_padding_mask=key_padding_mask\n        )\n        x = x + attn_output\n        x = self.layernorm(x)\n        return x\n","metadata":{"id":"zyXUSNlpwRT4","execution":{"iopub.status.busy":"2024-07-24T02:10:55.789990Z","iopub.execute_input":"2024-07-24T02:10:55.790303Z","iopub.status.idle":"2024-07-24T02:10:55.801681Z","shell.execute_reply.started":"2024-07-24T02:10:55.790280Z","shell.execute_reply":"2024-07-24T02:10:55.800848Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, dff, dropout_rate=0.1):\n        super(FeedForward, self).__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(d_model, dff),\n            nn.ReLU(),\n            nn.Linear(dff, d_model),\n            nn.Dropout(dropout_rate)\n        )\n        self.add = nn.Identity()\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        residual = x\n        x = self.seq(x)\n        x = self.add(residual + x)\n        x = self.layer_norm(x)\n        return x\n","metadata":{"id":"ESKszLNewU7i","execution":{"iopub.status.busy":"2024-07-24T02:10:55.802698Z","iopub.execute_input":"2024-07-24T02:10:55.802947Z","iopub.status.idle":"2024-07-24T02:10:55.818872Z","shell.execute_reply.started":"2024-07-24T02:10:55.802925Z","shell.execute_reply":"2024-07-24T02:10:55.818050Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, dff):\n        super(EncoderLayer, self).__init__()\n        self.self_attention = SelfAttention(embed_dim=embed_dim, num_heads=num_heads)\n        self.fn = FeedForward(d_model=embed_dim, dff=dff)\n\n    def forward(self, *input):\n        x = self.self_attention(*input)\n        x = self.fn(x)\n        return x\n","metadata":{"id":"7SpCY709wWWp","execution":{"iopub.status.busy":"2024-07-24T02:10:55.819992Z","iopub.execute_input":"2024-07-24T02:10:55.820246Z","iopub.status.idle":"2024-07-24T02:10:55.830495Z","shell.execute_reply.started":"2024-07-24T02:10:55.820224Z","shell.execute_reply":"2024-07-24T02:10:55.829731Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"id":"oO8o3lPXwffH"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass EncoderBERTpretrain(nn.Module):\n    def __init__(self, embedding_layer, encoder_layers, device) -> None:\n        super().__init__()\n        self.embedding_layer = embedding_layer.to(device)\n        self.encoder_layers: nn.ModuleList = encoder_layers.to(device)\n\n    def forward(self, input_phobert_ids):\n        input_ids = input_phobert_ids\n        attn_mask = input_ids != 1\n        attn_mask = attn_mask[:, None, None, :]\n        output = self.embedding_layer(input_ids)\n        for layer in self.encoder_layers:\n            output = layer(output, attention_mask=attn_mask)[0]\n        return output\n","metadata":{"id":"Z_XxRJL-wtos","execution":{"iopub.status.busy":"2024-07-24T02:10:55.831957Z","iopub.execute_input":"2024-07-24T02:10:55.832499Z","iopub.status.idle":"2024-07-24T02:10:55.843332Z","shell.execute_reply.started":"2024-07-24T02:10:55.832475Z","shell.execute_reply":"2024-07-24T02:10:55.842565Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n\nclass EncoderCharacter(nn.Module):\n    def __init__(\n        self,\n        num_layers,\n        num_heads,\n        embed_dim,\n        dff,\n        vocab_size,\n        device,\n        dropout_rate,\n    ) -> None:\n        super().__init__()\n        self.embedding_layer = PositionalEmbedding(\n            vocab_size=vocab_size, d_model=embed_dim\n        ).to(device=device)\n        self.encoder_layers = nn.ModuleList(\n            [\n                EncoderLayer(\n                    embed_dim=embed_dim,\n                    num_heads=num_heads,\n                    dropout=dropout_rate,\n                    dff=dff,\n                ).to(device)\n                for i in range(num_layers)\n            ]\n        )\n        self.device = device\n\n    def forward(self, *inputs):\n        # input_ids (batch_size x sentence_len x word_len)\n        input_ids, word_padding = inputs\n        batch_size, sentence_len, word_len = input_ids.size()\n        # input_ids ((batch_size * sentence_len) x word_len)\n        input_ids = input_ids.view(batch_size * sentence_len, word_len)\n        word_padding = word_padding.view(batch_size * sentence_len)\n        input_embed = input_ids[word_padding]\n        output = self.embedding_layer(input_embed)\n        key_mask_padding = input_embed == 1\n        for layer in self.encoder_layers:\n            output = layer(output, key_mask_padding)\n        output_padding = torch.zeros(\n            [batch_size * sentence_len, word_len, output.size(-1)]\n        ).to(device=self.device)\n        output_padding[word_padding] = output\n        output_padding.reshape(\n            batch_size, sentence_len, word_len, output_padding.size(-1)\n        )\n        output_padding = output_padding.mean(dim=1)\n        return output_padding\n","metadata":{"id":"cO2lczxQwwTG","execution":{"iopub.status.busy":"2024-07-24T02:10:55.844378Z","iopub.execute_input":"2024-07-24T02:10:55.844692Z","iopub.status.idle":"2024-07-24T02:10:55.861380Z","shell.execute_reply.started":"2024-07-24T02:10:55.844664Z","shell.execute_reply":"2024-07-24T02:10:55.860577Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# from custom_layer.EncoderCharacter import EncoderCharacter\n# from custom_layer.EncoderBERTpretrain import EncoderBERTpretrain\n# from transformer.EncoderLayer import EncoderLayer\n# from transformer.PositionalEmbedding import PositionalEmbedding\n\n# input: word_feature: (batch_size x sen_len x 768), chars_not_embedded: (batch_size x sen_len x word_len) -\n# output: (batch_size x sen_len)\n\n\nclass ModelSC(nn.Module):\n    def __init__(\n        self,\n        character_level_d_model,\n        word_level_d_model,\n        num_heads_char_encoder,\n        num_layers_char_encoder,\n        num_heads_word_encoder,\n        num_layers_word_encoder,\n        dff,\n        character_vocab_size,\n        word_vocab_size,\n        device,\n        dropout_rate=0.1,\n    ):\n        super().__init__()\n        self.encoder_character = EncoderCharacter(\n            num_heads=num_heads_char_encoder,\n            num_layers=num_layers_char_encoder,\n            vocab_size=character_vocab_size,\n            device=device,\n            dff=dff,\n            dropout_rate=dropout_rate,\n            embed_dim=character_level_d_model,\n        ).to(device)\n        self.embedding_word = PositionalEmbedding(\n            vocab_size=word_vocab_size, d_model=word_level_d_model\n        )\n        self.encoder_word = nn.ModuleList(\n            [\n                EncoderLayer(\n                    embed_dim=word_level_d_model,\n                    num_heads=num_heads_word_encoder,\n                    dropout=dropout_rate,\n                    dff=dff,\n                ).to(device)\n                for _ in range(num_layers_word_encoder)\n            ]\n        ).to(device=device)\n        self.linear = nn.Sequential(\n            nn.Linear(\n                character_level_d_model + word_level_d_model,\n                character_level_d_model + word_level_d_model,\n            ),\n            nn.LayerNorm(\n                character_level_d_model + word_level_d_model,\n                eps=1e-05,\n                elementwise_affine=True,\n            ),\n            nn.Linear(character_level_d_model + word_level_d_model, word_vocab_size),\n            nn.Dropout(p=dropout_rate),\n        )\n\n    def forward(self, *inputs):\n        # Input\n        char_input_ids, word_input_ids, masked_positions = inputs\n\n        # Encode character\n        key_mask_padding = word_input_ids != 1\n        output_char_encoder = self.encoder_character(char_input_ids, key_mask_padding)\n        output_char_encoder = output_char_encoder.view(\n            word_input_ids.size(0), word_input_ids.size(1), -1\n        )\n        # Encode word\n        output_word_encode = self.embedding_word(word_input_ids)\n        # return 1\n        for layer in self.encoder_word:\n            output_word_encode = layer(output_word_encode, word_input_ids == 1)\n        # Combine encode\n        output = torch.cat((output_word_encode, output_char_encoder), -1)\n        attn_mask = word_input_ids != 1\n        attn_mask = attn_mask[:, None, None, :]\n        output = output[masked_positions]\n        output = self.linear(output)\n        return output\n","metadata":{"id":"EOo2IOykwo17","execution":{"iopub.status.busy":"2024-07-24T02:10:55.862609Z","iopub.execute_input":"2024-07-24T02:10:55.862866Z","iopub.status.idle":"2024-07-24T02:10:55.881899Z","shell.execute_reply.started":"2024-07-24T02:10:55.862845Z","shell.execute_reply":"2024-07-24T02:10:55.881020Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass DetectModel(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        dff,\n        vocab_size,\n        num_layer,\n        device,\n        dropout_rate=0.1,\n    ):\n        super().__init__()\n        self.embedding_layer = PositionalEmbedding(\n            vocab_size=vocab_size, d_model=embed_dim\n        )\n        self.encoder_layers = nn.ModuleList(\n            [\n                EncoderLayer(\n                    embed_dim=embed_dim,\n                    num_heads=num_heads,\n                    dropout=dropout_rate,\n                    dff=dff,\n                ).to(device)\n                for _ in range(num_layer)\n            ]\n        )\n        self.classifier_layer = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.LayerNorm(\n                embed_dim,\n                eps=1e-05,\n                elementwise_affine=True,\n            ),\n            nn.Linear(embed_dim, 2, bias=True),\n            nn.Dropout(p=dropout_rate),\n            # nn.Softmax(dim=1),\n        )\n\n    def forward(self, x):\n        key_mask_padding = x == 1\n        x = self.embedding_layer(x)\n        for layer in self.encoder_layers:\n            x = layer(x, key_mask_padding)\n        return self.classifier_layer(x)\n","metadata":{"id":"s8hGFTegw3Vv","execution":{"iopub.status.busy":"2024-07-24T02:10:55.882905Z","iopub.execute_input":"2024-07-24T02:10:55.883377Z","iopub.status.idle":"2024-07-24T02:10:55.900608Z","shell.execute_reply.started":"2024-07-24T02:10:55.883354Z","shell.execute_reply":"2024-07-24T02:10:55.899901Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### trainning","metadata":{"id":"kusOs7sDyEgj"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 200\nword_embed_dim = 256\nchar_embed_dim = 256\ndevice0 = torch.device(\"cuda:0\")\ndevice1 = torch.device(\"cuda:1\")\nsentences = load_data_and_vocab(\n    path_file_data=\"/kaggle/input/datatrain-sc/sentences.txt\",\n    data_size=3000,\n)\ncustom_tokenizer = Tokenizer(vocab_file=\"/kaggle/input/datatrain-sc/vocab.txt\", unk_token=\"<unk>\")\nchar_tokenizer = Tokenizer(\n    vocab_file=\"/kaggle/input/datatrain-sc/char_vocab.txt\", unk_token=\"<unk>\", char_level=True\n)\ndata_builder = DataBuilder(\n    rng=random.Random(86),\n    padding_id=1,\n    mask_token=\"<mask>\",\n    seq_max_length=32,\n    word_max_length=10,\n    custom_tokenizer=custom_tokenizer,\n    char_tokenizer=char_tokenizer,\n)\n## Build data\nchar_input_ids, word_input_ids, mask_token_positions, target = data_builder.build_data(\n    inputs=sentences\n)\ndata_train = CustomDataset(\n    char_input_ids=char_input_ids,\n    word_input_ids=word_input_ids,\n    mask_token_positions=mask_token_positions,\n    target=target,\n)\ntrain_dataloader = DataLoader(\n    dataset=data_train,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=data_train.collate_fn,\n)\n\nmodel_detect = DetectModel(\n    embed_dim=768,\n    num_heads=4,\n    num_layer=12,\n    vocab_size=len(custom_tokenizer.vocab),\n    device=device1,\n    dff=2048,\n)\n\nmodel_fill = ModelSC(\n    character_level_d_model=char_embed_dim,\n    word_level_d_model=word_embed_dim,\n    num_heads_char_encoder=2,\n    num_heads_word_encoder=2,\n    num_layers_char_encoder=6,\n    num_layers_word_encoder=12,\n    dff=512,\n    character_vocab_size=len(char_tokenizer.vocab),\n    word_vocab_size=len(custom_tokenizer.vocab),\n    device=device0,\n)\n\n\ndef train_loop(\n    dataloader,\n    model_detect: DetectModel,\n    model_fill: ModelSC,\n    loss_fn_model_detect,\n    loss_fn_model_fill,\n    optimizer_model_detect,\n    optimizer_model_fill,\n    device0,\n    device1,\n):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model_detect.train()\n    model_fill.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Data for fill\n        word_input_ids_fill: torch.Tensor = X[\"word_input_ids\"].to(device0)\n        char_input_ids_fill = X[\"char_input_ids\"].to(device0)\n        mask_token_positions_fill = X[\"mask_token_positions\"].to(device0)\n\n        # Train model fill\n        pred_fill: torch.Tensor = model_fill(\n            char_input_ids_fill, word_input_ids_fill, mask_token_positions_fill\n        )\n        target_fill = (\n            torch.Tensor([item for sublist in y for item in sublist]).long().to(device0)\n        )\n        loss_fill = loss_fn_model_fill(pred_fill, target_fill)\n        loss_fill.backward(retain_graph=True)\n        optimizer_model_fill.step()\n        optimizer_model_fill.zero_grad()\n\n        # Data for detect\n        input_ids_detect = word_input_ids_fill.to(device1)\n#         word_input_ids_fill = word_input_ids_fill.to(device1)\n        mask_token_positions_fill = mask_token_positions_fill.to(device1)\n        pred_fill = pred_fill.to(device1)\n        input_ids_detect[mask_token_positions_fill] = pred_fill.argmax(-1)\n#         word_input_ids_fill[mask_token_positions_fill] = target_fill\n        target_detect = mask_token_positions_fill.to(device1).long()\n        target_detect = target_detect.view(\n            target_detect.shape[0] * target_detect.shape[1]\n        )\n        # Train model detect\n        pred_detect: torch.Tensor = model_detect(input_ids_detect)\n        pred_detect = pred_detect.view(\n            pred_detect.shape[0] * pred_detect.shape[1], pred_detect.shape[-1]\n        )\n        loss_detect = loss_fn_model_detect(pred_detect, target_detect)\n        loss_detect.backward()\n        optimizer_model_detect.step()\n        optimizer_model_detect.zero_grad()\n        if batch % 5 == 0:\n            loss_fill, current_fill = loss_fill.item(), batch * batch_size\n            loss_detect, current_detect = loss_detect.item(), batch * batch_size\n            print(\n                f\"loss: {loss_fill:>7f}  [{current_fill:>5d}/{size:>5d}]\",\n                f\"loss: {loss_detect:>7f}  [{current_detect:>5d}/{size:>5d}]\",\n            )\n\ndef train_loop_fill_masked(\n    dataloader,\n    model_fill: ModelSC,\n    loss_fn_model_fill,\n    optimizer_model_fill,\n    device,\n):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model_fill.train()\n    for batch, (X, y) in enumerate(dataloader):\n        word_input_ids_fill: torch.Tensor = X[\"word_input_ids\"].to(device)\n        char_input_ids_fill = X[\"char_input_ids\"].to(device)\n        mask_token_positions_fill = X[\"mask_token_positions\"].to(device)\n\n        # Train model fill\n        pred_fill: torch.Tensor = model_fill(\n            char_input_ids_fill, word_input_ids_fill, mask_token_positions_fill\n        )\n        target_fill = (\n            torch.Tensor([item for sublist in y for item in sublist]).long().to(device)\n        )\n        loss_fill = loss_fn_model_fill(pred_fill, target_fill)\n        loss_fill.backward(retain_graph=True)\n        optimizer_model_fill.step()\n        optimizer_model_fill.zero_grad()\n        if batch % 5 == 0:\n            loss_fill, current_fill = loss_fill.item(), batch * batch_size\n            # loss_detect, current_detect = loss_detect.item(), batch * batch_size\n            print(\n                f\"loss: {loss_fill:>7f}  [{current_fill:>5d}/{size:>5d}]\",\n                # f\"loss: {loss_detect:>7f}  [{current_detect:>5d}/{size:>5d}]\",\n            )\n            # for name, param in model_fill.named_parameters():\n            #     if param.grad is not None:\n            #         print(f\"Grad for {name}: {param.grad.norm().item()}\")\n\n            # # Check if weights are updating\n            # for name, param in model_fill.named_parameters():\n            #     print(f\"Weight for {name}: {param.norm().item()}\")\n","metadata":{"id":"IruVQeIMyG9U","execution":{"iopub.status.busy":"2024-07-24T02:10:55.901708Z","iopub.execute_input":"2024-07-24T02:10:55.901969Z","iopub.status.idle":"2024-07-24T02:11:10.043814Z","shell.execute_reply.started":"2024-07-24T02:10:55.901948Z","shell.execute_reply":"2024-07-24T02:11:10.042821Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# from prettytable import PrettyTable\n\n# def count_parameters(model):\n#     table = PrettyTable([\"Modules\", \"Parameters\"])\n#     total_params = 0\n#     for name, parameter in model.named_parameters():\n#         if not parameter.requires_grad:\n#             continue\n#         params = parameter.numel()\n#         table.add_row([name, params])\n#         total_params += params\n#     print(table)\n#     print(f\"Total Trainable Params: {total_params}\")\n#     return total_params\n\n# count_parameters(model_fill)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XoNPk1iG0IVH","outputId":"11755c72-780c-4452-9261-36064bc84516","execution":{"iopub.status.busy":"2024-07-24T02:11:10.045061Z","iopub.execute_input":"2024-07-24T02:11:10.045411Z","iopub.status.idle":"2024-07-24T02:11:10.050356Z","shell.execute_reply.started":"2024-07-24T02:11:10.045380Z","shell.execute_reply":"2024-07-24T02:11:10.049499Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-3\nepochs = 20\noptimizer_fill = torch.optim.Adam(model_fill.parameters(), lr=learning_rate)\noptimizer_detect = torch.optim.Adam(model_detect.parameters(), lr=learning_rate)\nloss_fn_fill = torch.nn.CrossEntropyLoss()\nloss_fn_detect = torch.nn.CrossEntropyLoss()\nmodel_fill.to(device0)\nmodel_detect.to(device1)\nfor t in range(epochs):\n    print(f\"Epoch {t + 1}\\n-------------------------------\")\n    train_loop(\n        model_detect=model_detect,\n        model_fill=model_fill,\n        dataloader=train_dataloader,\n        loss_fn_model_detect=loss_fn_detect,\n        loss_fn_model_fill=loss_fn_fill,\n        optimizer_model_detect=optimizer_detect,\n        optimizer_model_fill=optimizer_fill,\n        device0=device0,\n        device1=device1,\n    )\nprint(\"Done!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WXwQWNhyTAJ","outputId":"6667760d-3402-4bff-d264-496bb8610204","execution":{"iopub.status.busy":"2024-07-24T02:11:10.051614Z","iopub.execute_input":"2024-07-24T02:11:10.051959Z","iopub.status.idle":"2024-07-24T03:08:31.479207Z","shell.execute_reply.started":"2024-07-24T02:11:10.051927Z","shell.execute_reply":"2024-07-24T03:08:31.478277Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nloss: 9.309552  [    0/40538] loss: 1.065825  [    0/40538]\nloss: 6.895817  [ 1000/40538] loss: 1.167748  [ 1000/40538]\nloss: 6.505784  [ 2000/40538] loss: 0.379361  [ 2000/40538]\nloss: 6.255627  [ 3000/40538] loss: 0.416609  [ 3000/40538]\nloss: 6.175040  [ 4000/40538] loss: 0.379793  [ 4000/40538]\nloss: 6.285351  [ 5000/40538] loss: 0.381466  [ 5000/40538]\nloss: 6.087528  [ 6000/40538] loss: 0.364450  [ 6000/40538]\nloss: 6.127000  [ 7000/40538] loss: 0.371301  [ 7000/40538]\nloss: 5.875450  [ 8000/40538] loss: 0.359280  [ 8000/40538]\nloss: 6.022274  [ 9000/40538] loss: 0.381149  [ 9000/40538]\nloss: 5.777339  [10000/40538] loss: 0.370257  [10000/40538]\nloss: 5.833948  [11000/40538] loss: 0.369862  [11000/40538]\nloss: 5.733826  [12000/40538] loss: 0.362514  [12000/40538]\nloss: 5.569263  [13000/40538] loss: 0.361447  [13000/40538]\nloss: 5.561000  [14000/40538] loss: 0.365304  [14000/40538]\nloss: 5.774527  [15000/40538] loss: 0.371647  [15000/40538]\nloss: 5.766634  [16000/40538] loss: 0.367521  [16000/40538]\nloss: 5.557098  [17000/40538] loss: 0.371984  [17000/40538]\nloss: 5.480279  [18000/40538] loss: 0.382719  [18000/40538]\nloss: 5.402346  [19000/40538] loss: 0.364773  [19000/40538]\nloss: 5.331222  [20000/40538] loss: 0.373188  [20000/40538]\nloss: 5.265450  [21000/40538] loss: 0.355179  [21000/40538]\nloss: 5.234628  [22000/40538] loss: 0.357904  [22000/40538]\nloss: 5.552750  [23000/40538] loss: 0.376076  [23000/40538]\nloss: 5.236245  [24000/40538] loss: 0.359686  [24000/40538]\nloss: 5.220971  [25000/40538] loss: 0.359351  [25000/40538]\nloss: 5.339901  [26000/40538] loss: 0.372548  [26000/40538]\nloss: 5.205213  [27000/40538] loss: 0.362535  [27000/40538]\nloss: 4.844135  [28000/40538] loss: 0.351993  [28000/40538]\nloss: 5.232054  [29000/40538] loss: 0.366456  [29000/40538]\nloss: 5.087403  [30000/40538] loss: 0.368437  [30000/40538]\nloss: 5.021241  [31000/40538] loss: 0.360624  [31000/40538]\nloss: 5.014709  [32000/40538] loss: 0.367064  [32000/40538]\nloss: 5.077578  [33000/40538] loss: 0.363858  [33000/40538]\nloss: 5.162288  [34000/40538] loss: 0.372684  [34000/40538]\nloss: 4.852606  [35000/40538] loss: 0.363415  [35000/40538]\nloss: 4.843707  [36000/40538] loss: 0.357028  [36000/40538]\nloss: 4.936286  [37000/40538] loss: 0.367556  [37000/40538]\nloss: 4.841139  [38000/40538] loss: 0.357373  [38000/40538]\nloss: 5.050093  [39000/40538] loss: 0.369004  [39000/40538]\nloss: 4.537966  [40000/40538] loss: 0.365500  [40000/40538]\nEpoch 2\n-------------------------------\nloss: 4.849587  [    0/40538] loss: 0.369007  [    0/40538]\nloss: 4.767548  [ 1000/40538] loss: 0.366715  [ 1000/40538]\nloss: 4.892985  [ 2000/40538] loss: 0.374125  [ 2000/40538]\nloss: 4.773111  [ 3000/40538] loss: 0.357661  [ 3000/40538]\nloss: 4.849780  [ 4000/40538] loss: 0.354293  [ 4000/40538]\nloss: 4.917315  [ 5000/40538] loss: 0.360690  [ 5000/40538]\nloss: 4.751978  [ 6000/40538] loss: 0.355170  [ 6000/40538]\nloss: 4.875943  [ 7000/40538] loss: 0.366850  [ 7000/40538]\nloss: 4.812901  [ 8000/40538] loss: 0.369136  [ 8000/40538]\nloss: 4.755056  [ 9000/40538] loss: 0.365452  [ 9000/40538]\nloss: 4.721003  [10000/40538] loss: 0.360878  [10000/40538]\nloss: 4.607940  [11000/40538] loss: 0.353084  [11000/40538]\nloss: 4.795946  [12000/40538] loss: 0.379048  [12000/40538]\nloss: 4.773059  [13000/40538] loss: 0.368980  [13000/40538]\nloss: 4.783713  [14000/40538] loss: 0.372696  [14000/40538]\nloss: 4.884192  [15000/40538] loss: 0.364521  [15000/40538]\nloss: 4.712216  [16000/40538] loss: 0.368798  [16000/40538]\nloss: 4.759495  [17000/40538] loss: 0.363908  [17000/40538]\nloss: 4.789312  [18000/40538] loss: 0.372275  [18000/40538]\nloss: 4.934535  [19000/40538] loss: 0.373244  [19000/40538]\nloss: 4.818005  [20000/40538] loss: 0.368744  [20000/40538]\nloss: 4.438806  [21000/40538] loss: 0.349652  [21000/40538]\nloss: 4.629337  [22000/40538] loss: 0.355335  [22000/40538]\nloss: 4.768949  [23000/40538] loss: 0.374387  [23000/40538]\nloss: 4.448466  [24000/40538] loss: 0.355452  [24000/40538]\nloss: 4.735430  [25000/40538] loss: 0.381888  [25000/40538]\nloss: 4.812383  [26000/40538] loss: 0.362181  [26000/40538]\nloss: 4.435790  [27000/40538] loss: 0.363267  [27000/40538]\nloss: 4.878232  [28000/40538] loss: 0.363335  [28000/40538]\nloss: 4.713798  [29000/40538] loss: 0.366464  [29000/40538]\nloss: 4.877037  [30000/40538] loss: 0.356562  [30000/40538]\nloss: 4.627550  [31000/40538] loss: 0.367151  [31000/40538]\nloss: 4.729104  [32000/40538] loss: 0.370562  [32000/40538]\nloss: 4.769459  [33000/40538] loss: 0.370230  [33000/40538]\nloss: 4.606595  [34000/40538] loss: 0.358689  [34000/40538]\nloss: 4.936156  [35000/40538] loss: 0.369399  [35000/40538]\nloss: 4.700469  [36000/40538] loss: 0.361518  [36000/40538]\nloss: 4.777517  [37000/40538] loss: 0.366140  [37000/40538]\nloss: 4.647211  [38000/40538] loss: 0.355735  [38000/40538]\nloss: 4.684261  [39000/40538] loss: 0.362688  [39000/40538]\nloss: 4.647160  [40000/40538] loss: 0.361270  [40000/40538]\nEpoch 3\n-------------------------------\nloss: 4.633820  [    0/40538] loss: 0.374258  [    0/40538]\nloss: 4.744143  [ 1000/40538] loss: 0.359786  [ 1000/40538]\nloss: 4.621941  [ 2000/40538] loss: 0.375563  [ 2000/40538]\nloss: 4.554525  [ 3000/40538] loss: 0.366630  [ 3000/40538]\nloss: 4.364773  [ 4000/40538] loss: 0.364912  [ 4000/40538]\nloss: 4.628092  [ 5000/40538] loss: 0.378982  [ 5000/40538]\nloss: 4.479246  [ 6000/40538] loss: 0.355932  [ 6000/40538]\nloss: 4.516802  [ 7000/40538] loss: 0.362867  [ 7000/40538]\nloss: 4.732990  [ 8000/40538] loss: 0.369193  [ 8000/40538]\nloss: 4.393208  [ 9000/40538] loss: 0.367027  [ 9000/40538]\nloss: 4.607575  [10000/40538] loss: 0.362682  [10000/40538]\nloss: 4.300677  [11000/40538] loss: 0.356836  [11000/40538]\nloss: 4.511740  [12000/40538] loss: 0.356619  [12000/40538]\nloss: 4.760224  [13000/40538] loss: 0.372736  [13000/40538]\nloss: 4.452711  [14000/40538] loss: 0.364268  [14000/40538]\nloss: 4.696970  [15000/40538] loss: 0.368405  [15000/40538]\nloss: 4.652993  [16000/40538] loss: 0.365938  [16000/40538]\nloss: 4.636994  [17000/40538] loss: 0.372132  [17000/40538]\nloss: 4.170713  [18000/40538] loss: 0.350409  [18000/40538]\nloss: 4.400111  [19000/40538] loss: 0.349739  [19000/40538]\nloss: 4.576420  [20000/40538] loss: 0.362139  [20000/40538]\nloss: 4.539343  [21000/40538] loss: 0.364060  [21000/40538]\nloss: 4.508070  [22000/40538] loss: 0.358512  [22000/40538]\nloss: 4.381628  [23000/40538] loss: 0.358224  [23000/40538]\nloss: 4.473817  [24000/40538] loss: 0.357139  [24000/40538]\nloss: 4.323695  [25000/40538] loss: 0.359449  [25000/40538]\nloss: 4.435180  [26000/40538] loss: 0.354250  [26000/40538]\nloss: 4.528348  [27000/40538] loss: 0.363979  [27000/40538]\nloss: 4.599410  [28000/40538] loss: 0.370151  [28000/40538]\nloss: 4.661771  [29000/40538] loss: 0.359752  [29000/40538]\nloss: 4.536807  [30000/40538] loss: 0.358090  [30000/40538]\nloss: 4.303931  [31000/40538] loss: 0.359243  [31000/40538]\nloss: 4.388894  [32000/40538] loss: 0.371344  [32000/40538]\nloss: 4.555150  [33000/40538] loss: 0.364366  [33000/40538]\nloss: 4.456759  [34000/40538] loss: 0.361528  [34000/40538]\nloss: 4.430898  [35000/40538] loss: 0.363991  [35000/40538]\nloss: 4.326276  [36000/40538] loss: 0.353327  [36000/40538]\nloss: 4.537941  [37000/40538] loss: 0.365537  [37000/40538]\nloss: 4.568630  [38000/40538] loss: 0.370680  [38000/40538]\nloss: 4.472127  [39000/40538] loss: 0.365924  [39000/40538]\nloss: 4.590513  [40000/40538] loss: 0.368258  [40000/40538]\nEpoch 4\n-------------------------------\nloss: 4.367898  [    0/40538] loss: 0.364308  [    0/40538]\nloss: 4.363544  [ 1000/40538] loss: 0.374937  [ 1000/40538]\nloss: 4.601638  [ 2000/40538] loss: 0.368149  [ 2000/40538]\nloss: 4.393672  [ 3000/40538] loss: 0.362214  [ 3000/40538]\nloss: 4.523985  [ 4000/40538] loss: 0.375759  [ 4000/40538]\nloss: 4.294247  [ 5000/40538] loss: 0.355654  [ 5000/40538]\nloss: 4.498914  [ 6000/40538] loss: 0.363514  [ 6000/40538]\nloss: 4.386590  [ 7000/40538] loss: 0.356601  [ 7000/40538]\nloss: 4.211573  [ 8000/40538] loss: 0.365361  [ 8000/40538]\nloss: 4.432132  [ 9000/40538] loss: 0.361528  [ 9000/40538]\nloss: 4.290220  [10000/40538] loss: 0.366408  [10000/40538]\nloss: 4.495526  [11000/40538] loss: 0.366560  [11000/40538]\nloss: 4.331935  [12000/40538] loss: 0.356571  [12000/40538]\nloss: 4.693737  [13000/40538] loss: 0.380054  [13000/40538]\nloss: 4.584601  [14000/40538] loss: 0.370471  [14000/40538]\nloss: 4.466058  [15000/40538] loss: 0.371893  [15000/40538]\nloss: 4.328915  [16000/40538] loss: 0.375616  [16000/40538]\nloss: 4.416809  [17000/40538] loss: 0.355335  [17000/40538]\nloss: 4.326033  [18000/40538] loss: 0.367098  [18000/40538]\nloss: 4.160317  [19000/40538] loss: 0.350690  [19000/40538]\nloss: 4.561038  [20000/40538] loss: 0.377177  [20000/40538]\nloss: 4.468239  [21000/40538] loss: 0.354307  [21000/40538]\nloss: 4.457412  [22000/40538] loss: 0.363290  [22000/40538]\nloss: 4.601946  [23000/40538] loss: 0.373102  [23000/40538]\nloss: 4.440217  [24000/40538] loss: 0.361275  [24000/40538]\nloss: 4.248630  [25000/40538] loss: 0.354319  [25000/40538]\nloss: 4.459806  [26000/40538] loss: 0.357081  [26000/40538]\nloss: 4.580170  [27000/40538] loss: 0.369318  [27000/40538]\nloss: 4.586554  [28000/40538] loss: 0.367560  [28000/40538]\nloss: 4.349700  [29000/40538] loss: 0.355491  [29000/40538]\nloss: 4.639684  [30000/40538] loss: 0.374328  [30000/40538]\nloss: 4.329713  [31000/40538] loss: 0.364829  [31000/40538]\nloss: 4.535911  [32000/40538] loss: 0.375267  [32000/40538]\nloss: 4.465429  [33000/40538] loss: 0.366973  [33000/40538]\nloss: 4.361733  [34000/40538] loss: 0.361280  [34000/40538]\nloss: 4.388207  [35000/40538] loss: 0.362251  [35000/40538]\nloss: 4.173364  [36000/40538] loss: 0.359796  [36000/40538]\nloss: 4.376173  [37000/40538] loss: 0.359728  [37000/40538]\nloss: 4.486968  [38000/40538] loss: 0.368433  [38000/40538]\nloss: 4.381225  [39000/40538] loss: 0.355402  [39000/40538]\nloss: 4.536222  [40000/40538] loss: 0.377418  [40000/40538]\nEpoch 5\n-------------------------------\nloss: 4.394827  [    0/40538] loss: 0.361142  [    0/40538]\nloss: 4.239045  [ 1000/40538] loss: 0.355713  [ 1000/40538]\nloss: 4.231214  [ 2000/40538] loss: 0.359899  [ 2000/40538]\nloss: 4.675209  [ 3000/40538] loss: 0.367288  [ 3000/40538]\nloss: 4.383847  [ 4000/40538] loss: 0.357880  [ 4000/40538]\nloss: 4.311045  [ 5000/40538] loss: 0.366273  [ 5000/40538]\nloss: 4.329118  [ 6000/40538] loss: 0.362368  [ 6000/40538]\nloss: 4.299577  [ 7000/40538] loss: 0.356904  [ 7000/40538]\nloss: 4.417970  [ 8000/40538] loss: 0.367654  [ 8000/40538]\nloss: 4.263788  [ 9000/40538] loss: 0.357816  [ 9000/40538]\nloss: 4.290681  [10000/40538] loss: 0.355943  [10000/40538]\nloss: 4.244326  [11000/40538] loss: 0.358452  [11000/40538]\nloss: 4.414864  [12000/40538] loss: 0.360657  [12000/40538]\nloss: 4.119178  [13000/40538] loss: 0.354368  [13000/40538]\nloss: 4.439507  [14000/40538] loss: 0.362141  [14000/40538]\nloss: 4.321074  [15000/40538] loss: 0.366518  [15000/40538]\nloss: 4.269687  [16000/40538] loss: 0.359862  [16000/40538]\nloss: 4.256303  [17000/40538] loss: 0.364514  [17000/40538]\nloss: 4.341104  [18000/40538] loss: 0.359979  [18000/40538]\nloss: 4.024343  [19000/40538] loss: 0.353216  [19000/40538]\nloss: 4.453862  [20000/40538] loss: 0.374054  [20000/40538]\nloss: 4.224084  [21000/40538] loss: 0.355835  [21000/40538]\nloss: 4.428279  [22000/40538] loss: 0.367261  [22000/40538]\nloss: 4.316628  [23000/40538] loss: 0.353148  [23000/40538]\nloss: 4.281574  [24000/40538] loss: 0.359061  [24000/40538]\nloss: 4.296337  [25000/40538] loss: 0.359997  [25000/40538]\nloss: 4.535984  [26000/40538] loss: 0.373458  [26000/40538]\nloss: 4.298788  [27000/40538] loss: 0.362843  [27000/40538]\nloss: 4.534672  [28000/40538] loss: 0.375846  [28000/40538]\nloss: 4.372316  [29000/40538] loss: 0.362613  [29000/40538]\nloss: 4.385869  [30000/40538] loss: 0.362761  [30000/40538]\nloss: 4.439385  [31000/40538] loss: 0.376423  [31000/40538]\nloss: 4.477687  [32000/40538] loss: 0.364845  [32000/40538]\nloss: 4.264264  [33000/40538] loss: 0.361674  [33000/40538]\nloss: 4.320961  [34000/40538] loss: 0.366507  [34000/40538]\nloss: 4.243683  [35000/40538] loss: 0.357688  [35000/40538]\nloss: 4.033895  [36000/40538] loss: 0.350030  [36000/40538]\nloss: 4.319052  [37000/40538] loss: 0.362727  [37000/40538]\nloss: 4.454237  [38000/40538] loss: 0.376810  [38000/40538]\nloss: 4.421426  [39000/40538] loss: 0.370578  [39000/40538]\nloss: 4.210590  [40000/40538] loss: 0.359812  [40000/40538]\nEpoch 6\n-------------------------------\nloss: 4.210131  [    0/40538] loss: 0.360483  [    0/40538]\nloss: 4.193767  [ 1000/40538] loss: 0.367782  [ 1000/40538]\nloss: 4.359674  [ 2000/40538] loss: 0.376784  [ 2000/40538]\nloss: 4.236317  [ 3000/40538] loss: 0.370359  [ 3000/40538]\nloss: 4.381036  [ 4000/40538] loss: 0.372480  [ 4000/40538]\nloss: 3.987324  [ 5000/40538] loss: 0.353722  [ 5000/40538]\nloss: 4.430881  [ 6000/40538] loss: 0.369074  [ 6000/40538]\nloss: 4.267473  [ 7000/40538] loss: 0.365894  [ 7000/40538]\nloss: 4.116518  [ 8000/40538] loss: 0.356423  [ 8000/40538]\nloss: 4.172385  [ 9000/40538] loss: 0.363338  [ 9000/40538]\nloss: 4.209399  [10000/40538] loss: 0.363501  [10000/40538]\nloss: 4.222457  [11000/40538] loss: 0.370334  [11000/40538]\nloss: 4.202725  [12000/40538] loss: 0.347202  [12000/40538]\nloss: 4.277143  [13000/40538] loss: 0.359506  [13000/40538]\nloss: 4.395353  [14000/40538] loss: 0.367411  [14000/40538]\nloss: 4.090907  [15000/40538] loss: 0.349899  [15000/40538]\nloss: 4.280061  [16000/40538] loss: 0.359339  [16000/40538]\nloss: 4.161717  [17000/40538] loss: 0.356638  [17000/40538]\nloss: 4.442101  [18000/40538] loss: 0.372017  [18000/40538]\nloss: 4.207306  [19000/40538] loss: 0.361533  [19000/40538]\nloss: 4.538615  [20000/40538] loss: 0.374869  [20000/40538]\nloss: 4.250457  [21000/40538] loss: 0.365268  [21000/40538]\nloss: 4.391764  [22000/40538] loss: 0.364945  [22000/40538]\nloss: 4.299935  [23000/40538] loss: 0.374178  [23000/40538]\nloss: 4.401302  [24000/40538] loss: 0.362800  [24000/40538]\nloss: 4.298920  [25000/40538] loss: 0.355293  [25000/40538]\nloss: 4.319808  [26000/40538] loss: 0.357747  [26000/40538]\nloss: 4.251120  [27000/40538] loss: 0.354945  [27000/40538]\nloss: 4.372286  [28000/40538] loss: 0.366672  [28000/40538]\nloss: 4.242688  [29000/40538] loss: 0.368501  [29000/40538]\nloss: 4.250520  [30000/40538] loss: 0.356082  [30000/40538]\nloss: 4.379263  [31000/40538] loss: 0.362148  [31000/40538]\nloss: 4.073128  [32000/40538] loss: 0.360112  [32000/40538]\nloss: 4.091380  [33000/40538] loss: 0.357973  [33000/40538]\nloss: 4.304486  [34000/40538] loss: 0.367558  [34000/40538]\nloss: 4.397245  [35000/40538] loss: 0.366848  [35000/40538]\nloss: 4.400359  [36000/40538] loss: 0.361966  [36000/40538]\nloss: 4.254778  [37000/40538] loss: 0.359474  [37000/40538]\nloss: 4.320297  [38000/40538] loss: 0.374077  [38000/40538]\nloss: 4.239174  [39000/40538] loss: 0.358938  [39000/40538]\nloss: 4.356023  [40000/40538] loss: 0.372036  [40000/40538]\nEpoch 7\n-------------------------------\nloss: 4.421549  [    0/40538] loss: 0.372302  [    0/40538]\nloss: 4.362496  [ 1000/40538] loss: 0.368007  [ 1000/40538]\nloss: 4.264084  [ 2000/40538] loss: 0.366531  [ 2000/40538]\nloss: 4.281492  [ 3000/40538] loss: 0.354977  [ 3000/40538]\nloss: 4.308281  [ 4000/40538] loss: 0.356961  [ 4000/40538]\nloss: 4.088678  [ 5000/40538] loss: 0.358052  [ 5000/40538]\nloss: 4.187839  [ 6000/40538] loss: 0.353292  [ 6000/40538]\nloss: 4.247751  [ 7000/40538] loss: 0.366390  [ 7000/40538]\nloss: 4.083579  [ 8000/40538] loss: 0.358460  [ 8000/40538]\nloss: 4.238358  [ 9000/40538] loss: 0.357843  [ 9000/40538]\nloss: 4.351211  [10000/40538] loss: 0.356208  [10000/40538]\nloss: 3.962222  [11000/40538] loss: 0.356546  [11000/40538]\nloss: 4.165826  [12000/40538] loss: 0.359211  [12000/40538]\nloss: 4.314384  [13000/40538] loss: 0.374895  [13000/40538]\nloss: 4.077072  [14000/40538] loss: 0.367816  [14000/40538]\nloss: 4.196375  [15000/40538] loss: 0.374701  [15000/40538]\nloss: 4.396135  [16000/40538] loss: 0.373500  [16000/40538]\nloss: 3.857745  [17000/40538] loss: 0.335016  [17000/40538]\nloss: 4.009775  [18000/40538] loss: 0.353704  [18000/40538]\nloss: 4.276931  [19000/40538] loss: 0.368039  [19000/40538]\nloss: 4.381620  [20000/40538] loss: 0.364687  [20000/40538]\nloss: 4.485536  [21000/40538] loss: 0.365708  [21000/40538]\nloss: 4.177370  [22000/40538] loss: 0.353296  [22000/40538]\nloss: 4.385506  [23000/40538] loss: 0.365116  [23000/40538]\nloss: 4.326982  [24000/40538] loss: 0.375231  [24000/40538]\nloss: 4.343278  [25000/40538] loss: 0.369969  [25000/40538]\nloss: 4.203241  [26000/40538] loss: 0.365194  [26000/40538]\nloss: 4.262112  [27000/40538] loss: 0.363711  [27000/40538]\nloss: 4.179650  [28000/40538] loss: 0.362199  [28000/40538]\nloss: 4.352045  [29000/40538] loss: 0.365409  [29000/40538]\nloss: 4.347686  [30000/40538] loss: 0.370447  [30000/40538]\nloss: 4.231508  [31000/40538] loss: 0.365495  [31000/40538]\nloss: 4.147526  [32000/40538] loss: 0.353715  [32000/40538]\nloss: 3.945710  [33000/40538] loss: 0.340186  [33000/40538]\nloss: 4.280493  [34000/40538] loss: 0.374792  [34000/40538]\nloss: 4.391563  [35000/40538] loss: 0.368282  [35000/40538]\nloss: 4.141011  [36000/40538] loss: 0.357279  [36000/40538]\nloss: 4.287713  [37000/40538] loss: 0.365598  [37000/40538]\nloss: 4.210583  [38000/40538] loss: 0.354592  [38000/40538]\nloss: 4.294588  [39000/40538] loss: 0.363944  [39000/40538]\nloss: 4.200099  [40000/40538] loss: 0.365381  [40000/40538]\nEpoch 8\n-------------------------------\nloss: 4.304393  [    0/40538] loss: 0.370245  [    0/40538]\nloss: 4.367001  [ 1000/40538] loss: 0.367111  [ 1000/40538]\nloss: 4.079988  [ 2000/40538] loss: 0.355144  [ 2000/40538]\nloss: 4.325209  [ 3000/40538] loss: 0.373831  [ 3000/40538]\nloss: 4.343153  [ 4000/40538] loss: 0.361635  [ 4000/40538]\nloss: 4.149920  [ 5000/40538] loss: 0.364131  [ 5000/40538]\nloss: 4.263510  [ 6000/40538] loss: 0.357763  [ 6000/40538]\nloss: 4.365316  [ 7000/40538] loss: 0.379716  [ 7000/40538]\nloss: 4.222387  [ 8000/40538] loss: 0.370382  [ 8000/40538]\nloss: 4.167026  [ 9000/40538] loss: 0.369345  [ 9000/40538]\nloss: 4.416901  [10000/40538] loss: 0.361286  [10000/40538]\nloss: 4.190715  [11000/40538] loss: 0.360864  [11000/40538]\nloss: 4.163982  [12000/40538] loss: 0.358254  [12000/40538]\nloss: 4.106282  [13000/40538] loss: 0.365284  [13000/40538]\nloss: 4.291720  [14000/40538] loss: 0.368237  [14000/40538]\nloss: 4.376557  [15000/40538] loss: 0.371549  [15000/40538]\nloss: 4.552894  [16000/40538] loss: 0.375014  [16000/40538]\nloss: 4.136732  [17000/40538] loss: 0.357189  [17000/40538]\nloss: 4.039854  [18000/40538] loss: 0.365965  [18000/40538]\nloss: 4.311707  [19000/40538] loss: 0.365063  [19000/40538]\nloss: 4.714589  [20000/40538] loss: 0.375689  [20000/40538]\nloss: 4.311274  [21000/40538] loss: 0.360562  [21000/40538]\nloss: 4.046298  [22000/40538] loss: 0.351532  [22000/40538]\nloss: 4.255505  [23000/40538] loss: 0.369773  [23000/40538]\nloss: 4.126510  [24000/40538] loss: 0.372128  [24000/40538]\nloss: 4.319951  [25000/40538] loss: 0.372947  [25000/40538]\nloss: 4.204978  [26000/40538] loss: 0.363742  [26000/40538]\nloss: 4.312954  [27000/40538] loss: 0.364220  [27000/40538]\nloss: 4.308471  [28000/40538] loss: 0.374380  [28000/40538]\nloss: 4.188418  [29000/40538] loss: 0.363398  [29000/40538]\nloss: 4.219166  [30000/40538] loss: 0.366396  [30000/40538]\nloss: 4.466781  [31000/40538] loss: 0.368797  [31000/40538]\nloss: 3.948712  [32000/40538] loss: 0.351498  [32000/40538]\nloss: 4.232008  [33000/40538] loss: 0.363250  [33000/40538]\nloss: 4.409146  [34000/40538] loss: 0.373208  [34000/40538]\nloss: 3.992172  [35000/40538] loss: 0.355987  [35000/40538]\nloss: 4.472469  [36000/40538] loss: 0.372786  [36000/40538]\nloss: 4.240164  [37000/40538] loss: 0.356793  [37000/40538]\nloss: 4.168126  [38000/40538] loss: 0.362718  [38000/40538]\nloss: 4.331119  [39000/40538] loss: 0.359935  [39000/40538]\nloss: 4.289863  [40000/40538] loss: 0.376968  [40000/40538]\nEpoch 9\n-------------------------------\nloss: 4.230832  [    0/40538] loss: 0.361682  [    0/40538]\nloss: 4.469971  [ 1000/40538] loss: 0.375556  [ 1000/40538]\nloss: 3.991261  [ 2000/40538] loss: 0.343886  [ 2000/40538]\nloss: 4.500359  [ 3000/40538] loss: 0.371749  [ 3000/40538]\nloss: 4.208825  [ 4000/40538] loss: 0.364319  [ 4000/40538]\nloss: 4.184467  [ 5000/40538] loss: 0.362887  [ 5000/40538]\nloss: 4.255995  [ 6000/40538] loss: 0.356458  [ 6000/40538]\nloss: 4.094477  [ 7000/40538] loss: 0.366111  [ 7000/40538]\nloss: 4.324099  [ 8000/40538] loss: 0.362802  [ 8000/40538]\nloss: 3.849447  [ 9000/40538] loss: 0.354798  [ 9000/40538]\nloss: 4.252230  [10000/40538] loss: 0.361191  [10000/40538]\nloss: 4.194584  [11000/40538] loss: 0.368475  [11000/40538]\nloss: 4.271518  [12000/40538] loss: 0.375841  [12000/40538]\nloss: 4.226917  [13000/40538] loss: 0.378430  [13000/40538]\nloss: 4.256311  [14000/40538] loss: 0.368498  [14000/40538]\nloss: 4.477788  [15000/40538] loss: 0.376271  [15000/40538]\nloss: 4.285305  [16000/40538] loss: 0.363582  [16000/40538]\nloss: 4.207335  [17000/40538] loss: 0.369446  [17000/40538]\nloss: 4.308095  [18000/40538] loss: 0.365489  [18000/40538]\nloss: 4.278195  [19000/40538] loss: 0.365946  [19000/40538]\nloss: 4.343030  [20000/40538] loss: 0.369394  [20000/40538]\nloss: 4.185325  [21000/40538] loss: 0.355700  [21000/40538]\nloss: 4.219649  [22000/40538] loss: 0.375818  [22000/40538]\nloss: 4.337379  [23000/40538] loss: 0.376587  [23000/40538]\nloss: 4.098553  [24000/40538] loss: 0.349873  [24000/40538]\nloss: 4.207349  [25000/40538] loss: 0.360670  [25000/40538]\nloss: 3.942596  [26000/40538] loss: 0.360114  [26000/40538]\nloss: 4.172161  [27000/40538] loss: 0.365159  [27000/40538]\nloss: 4.283608  [28000/40538] loss: 0.373028  [28000/40538]\nloss: 4.141861  [29000/40538] loss: 0.366558  [29000/40538]\nloss: 4.201058  [30000/40538] loss: 0.364976  [30000/40538]\nloss: 4.288216  [31000/40538] loss: 0.374664  [31000/40538]\nloss: 4.314416  [32000/40538] loss: 0.365690  [32000/40538]\nloss: 4.269954  [33000/40538] loss: 0.367820  [33000/40538]\nloss: 4.175768  [34000/40538] loss: 0.362460  [34000/40538]\nloss: 4.074546  [35000/40538] loss: 0.355723  [35000/40538]\nloss: 4.300306  [36000/40538] loss: 0.374114  [36000/40538]\nloss: 4.407643  [37000/40538] loss: 0.359722  [37000/40538]\nloss: 4.382767  [38000/40538] loss: 0.369661  [38000/40538]\nloss: 3.968613  [39000/40538] loss: 0.345280  [39000/40538]\nloss: 3.939457  [40000/40538] loss: 0.352191  [40000/40538]\nEpoch 10\n-------------------------------\nloss: 4.118917  [    0/40538] loss: 0.375594  [    0/40538]\nloss: 4.244948  [ 1000/40538] loss: 0.375893  [ 1000/40538]\nloss: 4.036504  [ 2000/40538] loss: 0.365043  [ 2000/40538]\nloss: 4.281617  [ 3000/40538] loss: 0.374954  [ 3000/40538]\nloss: 4.150567  [ 4000/40538] loss: 0.362296  [ 4000/40538]\nloss: 3.930260  [ 5000/40538] loss: 0.361218  [ 5000/40538]\nloss: 4.191207  [ 6000/40538] loss: 0.364849  [ 6000/40538]\nloss: 4.311633  [ 7000/40538] loss: 0.374568  [ 7000/40538]\nloss: 4.226147  [ 8000/40538] loss: 0.370219  [ 8000/40538]\nloss: 4.083481  [ 9000/40538] loss: 0.357707  [ 9000/40538]\nloss: 4.037467  [10000/40538] loss: 0.364702  [10000/40538]\nloss: 4.242585  [11000/40538] loss: 0.368798  [11000/40538]\nloss: 4.147203  [12000/40538] loss: 0.368382  [12000/40538]\nloss: 4.191463  [13000/40538] loss: 0.366283  [13000/40538]\nloss: 4.317490  [14000/40538] loss: 0.369067  [14000/40538]\nloss: 4.232441  [15000/40538] loss: 0.373251  [15000/40538]\nloss: 4.242146  [16000/40538] loss: 0.371755  [16000/40538]\nloss: 3.965239  [17000/40538] loss: 0.350200  [17000/40538]\nloss: 4.336718  [18000/40538] loss: 0.367094  [18000/40538]\nloss: 4.165917  [19000/40538] loss: 0.364199  [19000/40538]\nloss: 4.303922  [20000/40538] loss: 0.367627  [20000/40538]\nloss: 4.181288  [21000/40538] loss: 0.355002  [21000/40538]\nloss: 4.303270  [22000/40538] loss: 0.371860  [22000/40538]\nloss: 4.106208  [23000/40538] loss: 0.362279  [23000/40538]\nloss: 4.233549  [24000/40538] loss: 0.361640  [24000/40538]\nloss: 4.479442  [25000/40538] loss: 0.368143  [25000/40538]\nloss: 4.360067  [26000/40538] loss: 0.384275  [26000/40538]\nloss: 4.184509  [27000/40538] loss: 0.372483  [27000/40538]\nloss: 4.070822  [28000/40538] loss: 0.371863  [28000/40538]\nloss: 4.224103  [29000/40538] loss: 0.361552  [29000/40538]\nloss: 4.221105  [30000/40538] loss: 0.367273  [30000/40538]\nloss: 4.263701  [31000/40538] loss: 0.371902  [31000/40538]\nloss: 4.174491  [32000/40538] loss: 0.358344  [32000/40538]\nloss: 4.059298  [33000/40538] loss: 0.357175  [33000/40538]\nloss: 4.094172  [34000/40538] loss: 0.363675  [34000/40538]\nloss: 4.073477  [35000/40538] loss: 0.351652  [35000/40538]\nloss: 4.275346  [36000/40538] loss: 0.376756  [36000/40538]\nloss: 4.242832  [37000/40538] loss: 0.360148  [37000/40538]\nloss: 3.946954  [38000/40538] loss: 0.356363  [38000/40538]\nloss: 4.178366  [39000/40538] loss: 0.357131  [39000/40538]\nloss: 4.443635  [40000/40538] loss: 0.371819  [40000/40538]\nEpoch 11\n-------------------------------\nloss: 4.157665  [    0/40538] loss: 0.367922  [    0/40538]\nloss: 4.027678  [ 1000/40538] loss: 0.357328  [ 1000/40538]\nloss: 3.912518  [ 2000/40538] loss: 0.350929  [ 2000/40538]\nloss: 4.391592  [ 3000/40538] loss: 0.373191  [ 3000/40538]\nloss: 4.121811  [ 4000/40538] loss: 0.365278  [ 4000/40538]\nloss: 4.187042  [ 5000/40538] loss: 0.363555  [ 5000/40538]\nloss: 3.974690  [ 6000/40538] loss: 0.360036  [ 6000/40538]\nloss: 4.195502  [ 7000/40538] loss: 0.357177  [ 7000/40538]\nloss: 4.198178  [ 8000/40538] loss: 0.378712  [ 8000/40538]\nloss: 4.094056  [ 9000/40538] loss: 0.352648  [ 9000/40538]\nloss: 4.051157  [10000/40538] loss: 0.361355  [10000/40538]\nloss: 4.246021  [11000/40538] loss: 0.368064  [11000/40538]\nloss: 4.232751  [12000/40538] loss: 0.362881  [12000/40538]\nloss: 4.025677  [13000/40538] loss: 0.356535  [13000/40538]\nloss: 4.231099  [14000/40538] loss: 0.364265  [14000/40538]\nloss: 4.083492  [15000/40538] loss: 0.362846  [15000/40538]\nloss: 4.136167  [16000/40538] loss: 0.361380  [16000/40538]\nloss: 4.115261  [17000/40538] loss: 0.367320  [17000/40538]\nloss: 4.201036  [18000/40538] loss: 0.366463  [18000/40538]\nloss: 4.062783  [19000/40538] loss: 0.359094  [19000/40538]\nloss: 4.360850  [20000/40538] loss: 0.372039  [20000/40538]\nloss: 4.175309  [21000/40538] loss: 0.361293  [21000/40538]\nloss: 4.062696  [22000/40538] loss: 0.363329  [22000/40538]\nloss: 4.105083  [23000/40538] loss: 0.365725  [23000/40538]\nloss: 3.882221  [24000/40538] loss: 0.351978  [24000/40538]\nloss: 3.971516  [25000/40538] loss: 0.354078  [25000/40538]\nloss: 3.972832  [26000/40538] loss: 0.349409  [26000/40538]\nloss: 4.049858  [27000/40538] loss: 0.361805  [27000/40538]\nloss: 4.307752  [28000/40538] loss: 0.373411  [28000/40538]\nloss: 4.282467  [29000/40538] loss: 0.366564  [29000/40538]\nloss: 4.166868  [30000/40538] loss: 0.361591  [30000/40538]\nloss: 4.109421  [31000/40538] loss: 0.358721  [31000/40538]\nloss: 4.407714  [32000/40538] loss: 0.372228  [32000/40538]\nloss: 4.090128  [33000/40538] loss: 0.358255  [33000/40538]\nloss: 4.175148  [34000/40538] loss: 0.354959  [34000/40538]\nloss: 4.261659  [35000/40538] loss: 0.361835  [35000/40538]\nloss: 4.401649  [36000/40538] loss: 0.376662  [36000/40538]\nloss: 4.206469  [37000/40538] loss: 0.360005  [37000/40538]\nloss: 4.289414  [38000/40538] loss: 0.372308  [38000/40538]\nloss: 4.345818  [39000/40538] loss: 0.362117  [39000/40538]\nloss: 3.950903  [40000/40538] loss: 0.347205  [40000/40538]\nEpoch 12\n-------------------------------\nloss: 4.264947  [    0/40538] loss: 0.370851  [    0/40538]\nloss: 4.261854  [ 1000/40538] loss: 0.369820  [ 1000/40538]\nloss: 4.201349  [ 2000/40538] loss: 0.362505  [ 2000/40538]\nloss: 4.304066  [ 3000/40538] loss: 0.375018  [ 3000/40538]\nloss: 4.129967  [ 4000/40538] loss: 0.372924  [ 4000/40538]\nloss: 4.186071  [ 5000/40538] loss: 0.364714  [ 5000/40538]\nloss: 3.953390  [ 6000/40538] loss: 0.354962  [ 6000/40538]\nloss: 4.176201  [ 7000/40538] loss: 0.365969  [ 7000/40538]\nloss: 4.175084  [ 8000/40538] loss: 0.375023  [ 8000/40538]\nloss: 4.113291  [ 9000/40538] loss: 0.359392  [ 9000/40538]\nloss: 4.273291  [10000/40538] loss: 0.364138  [10000/40538]\nloss: 4.230181  [11000/40538] loss: 0.361621  [11000/40538]\nloss: 4.169703  [12000/40538] loss: 0.365665  [12000/40538]\nloss: 4.503069  [13000/40538] loss: 0.372708  [13000/40538]\nloss: 4.129436  [14000/40538] loss: 0.359645  [14000/40538]\nloss: 4.092602  [15000/40538] loss: 0.357098  [15000/40538]\nloss: 4.105723  [16000/40538] loss: 0.362071  [16000/40538]\nloss: 4.231826  [17000/40538] loss: 0.355845  [17000/40538]\nloss: 4.143390  [18000/40538] loss: 0.348381  [18000/40538]\nloss: 4.098318  [19000/40538] loss: 0.361262  [19000/40538]\nloss: 3.985489  [20000/40538] loss: 0.356320  [20000/40538]\nloss: 4.150854  [21000/40538] loss: 0.363791  [21000/40538]\nloss: 4.119311  [22000/40538] loss: 0.361616  [22000/40538]\nloss: 3.987886  [23000/40538] loss: 0.355510  [23000/40538]\nloss: 4.111967  [24000/40538] loss: 0.361652  [24000/40538]\nloss: 4.149105  [25000/40538] loss: 0.354134  [25000/40538]\nloss: 4.315233  [26000/40538] loss: 0.378416  [26000/40538]\nloss: 4.343328  [27000/40538] loss: 0.372646  [27000/40538]\nloss: 4.182586  [28000/40538] loss: 0.376399  [28000/40538]\nloss: 4.508432  [29000/40538] loss: 0.377150  [29000/40538]\nloss: 4.353133  [30000/40538] loss: 0.374169  [30000/40538]\nloss: 4.036965  [31000/40538] loss: 0.361823  [31000/40538]\nloss: 4.151142  [32000/40538] loss: 0.351786  [32000/40538]\nloss: 4.158701  [33000/40538] loss: 0.369979  [33000/40538]\nloss: 4.176382  [34000/40538] loss: 0.358439  [34000/40538]\nloss: 4.473316  [35000/40538] loss: 0.375356  [35000/40538]\nloss: 4.361832  [36000/40538] loss: 0.362231  [36000/40538]\nloss: 4.046746  [37000/40538] loss: 0.363132  [37000/40538]\nloss: 4.213082  [38000/40538] loss: 0.365996  [38000/40538]\nloss: 4.194440  [39000/40538] loss: 0.363545  [39000/40538]\nloss: 4.009520  [40000/40538] loss: 0.356777  [40000/40538]\nEpoch 13\n-------------------------------\nloss: 3.920036  [    0/40538] loss: 0.357516  [    0/40538]\nloss: 3.927830  [ 1000/40538] loss: 0.353506  [ 1000/40538]\nloss: 3.883620  [ 2000/40538] loss: 0.350574  [ 2000/40538]\nloss: 3.913575  [ 3000/40538] loss: 0.361923  [ 3000/40538]\nloss: 4.217357  [ 4000/40538] loss: 0.365491  [ 4000/40538]\nloss: 4.157331  [ 5000/40538] loss: 0.375270  [ 5000/40538]\nloss: 4.200633  [ 6000/40538] loss: 0.369557  [ 6000/40538]\nloss: 4.320634  [ 7000/40538] loss: 0.367307  [ 7000/40538]\nloss: 4.181903  [ 8000/40538] loss: 0.368522  [ 8000/40538]\nloss: 4.150078  [ 9000/40538] loss: 0.360346  [ 9000/40538]\nloss: 4.249352  [10000/40538] loss: 0.363886  [10000/40538]\nloss: 4.064793  [11000/40538] loss: 0.360543  [11000/40538]\nloss: 4.034517  [12000/40538] loss: 0.363912  [12000/40538]\nloss: 4.351587  [13000/40538] loss: 0.373829  [13000/40538]\nloss: 4.083179  [14000/40538] loss: 0.353710  [14000/40538]\nloss: 4.196837  [15000/40538] loss: 0.358104  [15000/40538]\nloss: 4.237805  [16000/40538] loss: 0.368291  [16000/40538]\nloss: 4.003983  [17000/40538] loss: 0.360910  [17000/40538]\nloss: 4.062946  [18000/40538] loss: 0.356629  [18000/40538]\nloss: 4.196271  [19000/40538] loss: 0.364776  [19000/40538]\nloss: 3.916122  [20000/40538] loss: 0.344095  [20000/40538]\nloss: 4.461249  [21000/40538] loss: 0.364945  [21000/40538]\nloss: 4.270243  [22000/40538] loss: 0.368778  [22000/40538]\nloss: 3.953535  [23000/40538] loss: 0.357240  [23000/40538]\nloss: 4.122879  [24000/40538] loss: 0.356628  [24000/40538]\nloss: 4.131049  [25000/40538] loss: 0.363508  [25000/40538]\nloss: 4.048197  [26000/40538] loss: 0.366637  [26000/40538]\nloss: 4.093047  [27000/40538] loss: 0.368740  [27000/40538]\nloss: 4.316163  [28000/40538] loss: 0.362173  [28000/40538]\nloss: 4.001409  [29000/40538] loss: 0.364126  [29000/40538]\nloss: 4.206186  [30000/40538] loss: 0.364275  [30000/40538]\nloss: 4.330068  [31000/40538] loss: 0.365762  [31000/40538]\nloss: 4.128107  [32000/40538] loss: 0.359896  [32000/40538]\nloss: 4.181058  [33000/40538] loss: 0.358755  [33000/40538]\nloss: 4.161052  [34000/40538] loss: 0.354955  [34000/40538]\nloss: 4.139931  [35000/40538] loss: 0.367733  [35000/40538]\nloss: 4.263209  [36000/40538] loss: 0.362680  [36000/40538]\nloss: 4.326405  [37000/40538] loss: 0.367799  [37000/40538]\nloss: 4.255650  [38000/40538] loss: 0.367783  [38000/40538]\nloss: 4.396692  [39000/40538] loss: 0.375132  [39000/40538]\nloss: 4.021809  [40000/40538] loss: 0.365993  [40000/40538]\nEpoch 14\n-------------------------------\nloss: 4.104796  [    0/40538] loss: 0.366932  [    0/40538]\nloss: 4.137628  [ 1000/40538] loss: 0.364487  [ 1000/40538]\nloss: 4.067467  [ 2000/40538] loss: 0.368670  [ 2000/40538]\nloss: 4.155560  [ 3000/40538] loss: 0.375131  [ 3000/40538]\nloss: 4.135533  [ 4000/40538] loss: 0.365138  [ 4000/40538]\nloss: 4.132834  [ 5000/40538] loss: 0.357551  [ 5000/40538]\nloss: 4.275129  [ 6000/40538] loss: 0.367135  [ 6000/40538]\nloss: 4.314207  [ 7000/40538] loss: 0.378454  [ 7000/40538]\nloss: 4.369359  [ 8000/40538] loss: 0.382951  [ 8000/40538]\nloss: 4.111884  [ 9000/40538] loss: 0.368496  [ 9000/40538]\nloss: 4.010344  [10000/40538] loss: 0.353531  [10000/40538]\nloss: 4.126531  [11000/40538] loss: 0.359462  [11000/40538]\nloss: 4.019519  [12000/40538] loss: 0.360468  [12000/40538]\nloss: 4.488548  [13000/40538] loss: 0.381413  [13000/40538]\nloss: 4.272401  [14000/40538] loss: 0.371790  [14000/40538]\nloss: 4.168070  [15000/40538] loss: 0.371157  [15000/40538]\nloss: 3.994125  [16000/40538] loss: 0.362026  [16000/40538]\nloss: 4.164598  [17000/40538] loss: 0.353414  [17000/40538]\nloss: 4.260986  [18000/40538] loss: 0.369607  [18000/40538]\nloss: 4.138458  [19000/40538] loss: 0.359437  [19000/40538]\nloss: 4.161202  [20000/40538] loss: 0.365159  [20000/40538]\nloss: 4.007159  [21000/40538] loss: 0.360827  [21000/40538]\nloss: 4.148956  [22000/40538] loss: 0.356353  [22000/40538]\nloss: 4.436944  [23000/40538] loss: 0.375484  [23000/40538]\nloss: 4.054804  [24000/40538] loss: 0.360971  [24000/40538]\nloss: 4.269907  [25000/40538] loss: 0.364452  [25000/40538]\nloss: 4.301654  [26000/40538] loss: 0.361002  [26000/40538]\nloss: 4.322356  [27000/40538] loss: 0.369679  [27000/40538]\nloss: 4.095455  [28000/40538] loss: 0.363751  [28000/40538]\nloss: 4.358405  [29000/40538] loss: 0.370807  [29000/40538]\nloss: 4.052498  [30000/40538] loss: 0.355504  [30000/40538]\nloss: 4.108505  [31000/40538] loss: 0.363457  [31000/40538]\nloss: 4.287333  [32000/40538] loss: 0.367092  [32000/40538]\nloss: 4.325610  [33000/40538] loss: 0.366656  [33000/40538]\nloss: 4.204496  [34000/40538] loss: 0.365813  [34000/40538]\nloss: 4.458834  [35000/40538] loss: 0.362816  [35000/40538]\nloss: 4.397992  [36000/40538] loss: 0.373772  [36000/40538]\nloss: 4.226953  [37000/40538] loss: 0.366560  [37000/40538]\nloss: 4.729362  [38000/40538] loss: 0.377323  [38000/40538]\nloss: 4.390110  [39000/40538] loss: 0.362068  [39000/40538]\nloss: 4.269980  [40000/40538] loss: 0.369776  [40000/40538]\nEpoch 15\n-------------------------------\nloss: 4.049666  [    0/40538] loss: 0.355369  [    0/40538]\nloss: 4.165495  [ 1000/40538] loss: 0.370939  [ 1000/40538]\nloss: 4.480544  [ 2000/40538] loss: 0.369173  [ 2000/40538]\nloss: 3.946478  [ 3000/40538] loss: 0.353168  [ 3000/40538]\nloss: 4.238000  [ 4000/40538] loss: 0.365930  [ 4000/40538]\nloss: 4.239286  [ 5000/40538] loss: 0.367240  [ 5000/40538]\nloss: 4.138926  [ 6000/40538] loss: 0.356984  [ 6000/40538]\nloss: 4.158123  [ 7000/40538] loss: 0.368882  [ 7000/40538]\nloss: 4.232442  [ 8000/40538] loss: 0.373978  [ 8000/40538]\nloss: 3.967793  [ 9000/40538] loss: 0.360601  [ 9000/40538]\nloss: 4.412452  [10000/40538] loss: 0.363749  [10000/40538]\nloss: 4.287115  [11000/40538] loss: 0.366776  [11000/40538]\nloss: 4.465414  [12000/40538] loss: 0.377500  [12000/40538]\nloss: 4.249608  [13000/40538] loss: 0.355599  [13000/40538]\nloss: 4.391003  [14000/40538] loss: 0.373651  [14000/40538]\nloss: 4.482990  [15000/40538] loss: 0.388353  [15000/40538]\nloss: 4.039079  [16000/40538] loss: 0.349450  [16000/40538]\nloss: 4.164423  [17000/40538] loss: 0.365268  [17000/40538]\nloss: 4.440570  [18000/40538] loss: 0.362505  [18000/40538]\nloss: 4.460056  [19000/40538] loss: 0.367755  [19000/40538]\nloss: 4.139430  [20000/40538] loss: 0.365325  [20000/40538]\nloss: 4.153302  [21000/40538] loss: 0.359469  [21000/40538]\nloss: 4.221599  [22000/40538] loss: 0.362667  [22000/40538]\nloss: 4.235289  [23000/40538] loss: 0.366844  [23000/40538]\nloss: 4.328586  [24000/40538] loss: 0.363205  [24000/40538]\nloss: 4.364110  [25000/40538] loss: 0.374608  [25000/40538]\nloss: 4.270959  [26000/40538] loss: 0.373676  [26000/40538]\nloss: 4.089863  [27000/40538] loss: 0.354905  [27000/40538]\nloss: 4.322526  [28000/40538] loss: 0.364084  [28000/40538]\nloss: 4.318663  [29000/40538] loss: 0.375948  [29000/40538]\nloss: 4.415274  [30000/40538] loss: 0.379419  [30000/40538]\nloss: 4.291767  [31000/40538] loss: 0.357534  [31000/40538]\nloss: 4.154155  [32000/40538] loss: 0.354335  [32000/40538]\nloss: 4.285022  [33000/40538] loss: 0.373358  [33000/40538]\nloss: 4.402434  [34000/40538] loss: 0.371158  [34000/40538]\nloss: 4.135323  [35000/40538] loss: 0.353898  [35000/40538]\nloss: 4.357508  [36000/40538] loss: 0.369571  [36000/40538]\nloss: 4.162714  [37000/40538] loss: 0.363444  [37000/40538]\nloss: 4.282905  [38000/40538] loss: 0.363989  [38000/40538]\nloss: 4.018430  [39000/40538] loss: 0.354915  [39000/40538]\nloss: 4.473476  [40000/40538] loss: 0.373329  [40000/40538]\nEpoch 16\n-------------------------------\nloss: 4.080688  [    0/40538] loss: 0.359190  [    0/40538]\nloss: 4.210485  [ 1000/40538] loss: 0.367688  [ 1000/40538]\nloss: 3.928335  [ 2000/40538] loss: 0.365017  [ 2000/40538]\nloss: 4.292511  [ 3000/40538] loss: 0.370413  [ 3000/40538]\nloss: 4.281807  [ 4000/40538] loss: 0.366017  [ 4000/40538]\nloss: 4.009225  [ 5000/40538] loss: 0.370898  [ 5000/40538]\nloss: 4.129726  [ 6000/40538] loss: 0.364077  [ 6000/40538]\nloss: 4.264948  [ 7000/40538] loss: 0.374346  [ 7000/40538]\nloss: 4.223839  [ 8000/40538] loss: 0.369237  [ 8000/40538]\nloss: 4.301883  [ 9000/40538] loss: 0.372229  [ 9000/40538]\nloss: 4.200772  [10000/40538] loss: 0.356674  [10000/40538]\nloss: 4.462926  [11000/40538] loss: 0.373113  [11000/40538]\nloss: 4.422814  [12000/40538] loss: 0.377168  [12000/40538]\nloss: 4.374847  [13000/40538] loss: 0.373085  [13000/40538]\nloss: 4.312323  [14000/40538] loss: 0.366779  [14000/40538]\nloss: 4.104198  [15000/40538] loss: 0.374290  [15000/40538]\nloss: 4.268350  [16000/40538] loss: 0.373186  [16000/40538]\nloss: 4.062508  [17000/40538] loss: 0.368829  [17000/40538]\nloss: 4.238200  [18000/40538] loss: 0.363627  [18000/40538]\nloss: 4.275196  [19000/40538] loss: 0.371530  [19000/40538]\nloss: 4.323276  [20000/40538] loss: 0.365647  [20000/40538]\nloss: 4.145755  [21000/40538] loss: 0.358135  [21000/40538]\nloss: 4.098008  [22000/40538] loss: 0.356825  [22000/40538]\nloss: 4.223769  [23000/40538] loss: 0.366259  [23000/40538]\nloss: 3.817961  [24000/40538] loss: 0.351755  [24000/40538]\nloss: 4.005693  [25000/40538] loss: 0.354781  [25000/40538]\nloss: 4.285198  [26000/40538] loss: 0.366851  [26000/40538]\nloss: 4.277070  [27000/40538] loss: 0.360818  [27000/40538]\nloss: 4.263382  [28000/40538] loss: 0.369841  [28000/40538]\nloss: 4.189850  [29000/40538] loss: 0.355931  [29000/40538]\nloss: 4.212750  [30000/40538] loss: 0.360573  [30000/40538]\nloss: 4.148266  [31000/40538] loss: 0.346658  [31000/40538]\nloss: 4.306066  [32000/40538] loss: 0.367011  [32000/40538]\nloss: 4.381109  [33000/40538] loss: 0.362929  [33000/40538]\nloss: 4.074215  [34000/40538] loss: 0.361122  [34000/40538]\nloss: 4.056541  [35000/40538] loss: 0.354439  [35000/40538]\nloss: 4.470990  [36000/40538] loss: 0.375582  [36000/40538]\nloss: 4.347437  [37000/40538] loss: 0.374691  [37000/40538]\nloss: 4.147863  [38000/40538] loss: 0.365464  [38000/40538]\nloss: 4.279216  [39000/40538] loss: 0.372561  [39000/40538]\nloss: 3.989897  [40000/40538] loss: 0.361316  [40000/40538]\nEpoch 17\n-------------------------------\nloss: 4.078019  [    0/40538] loss: 0.360204  [    0/40538]\nloss: 4.224601  [ 1000/40538] loss: 0.359270  [ 1000/40538]\nloss: 3.906274  [ 2000/40538] loss: 0.354086  [ 2000/40538]\nloss: 4.368009  [ 3000/40538] loss: 0.367845  [ 3000/40538]\nloss: 4.174772  [ 4000/40538] loss: 0.365541  [ 4000/40538]\nloss: 3.899664  [ 5000/40538] loss: 0.351000  [ 5000/40538]\nloss: 4.149470  [ 6000/40538] loss: 0.360356  [ 6000/40538]\nloss: 4.020777  [ 7000/40538] loss: 0.356410  [ 7000/40538]\nloss: 4.187670  [ 8000/40538] loss: 0.369278  [ 8000/40538]\nloss: 4.241183  [ 9000/40538] loss: 0.370298  [ 9000/40538]\nloss: 4.319444  [10000/40538] loss: 0.359292  [10000/40538]\nloss: 4.071619  [11000/40538] loss: 0.360335  [11000/40538]\nloss: 4.099491  [12000/40538] loss: 0.364557  [12000/40538]\nloss: 4.390130  [13000/40538] loss: 0.369209  [13000/40538]\nloss: 4.218239  [14000/40538] loss: 0.360810  [14000/40538]\nloss: 4.123602  [15000/40538] loss: 0.372972  [15000/40538]\nloss: 3.878589  [16000/40538] loss: 0.358731  [16000/40538]\nloss: 4.486890  [17000/40538] loss: 0.371599  [17000/40538]\nloss: 4.165345  [18000/40538] loss: 0.359681  [18000/40538]\nloss: 4.095547  [19000/40538] loss: 0.366557  [19000/40538]\nloss: 4.130602  [20000/40538] loss: 0.361597  [20000/40538]\nloss: 3.852663  [21000/40538] loss: 0.358240  [21000/40538]\nloss: 4.154435  [22000/40538] loss: 0.365032  [22000/40538]\nloss: 4.218238  [23000/40538] loss: 0.360218  [23000/40538]\nloss: 3.873758  [24000/40538] loss: 0.362967  [24000/40538]\nloss: 4.257357  [25000/40538] loss: 0.369870  [25000/40538]\nloss: 4.068608  [26000/40538] loss: 0.356696  [26000/40538]\nloss: 4.200086  [27000/40538] loss: 0.355454  [27000/40538]\nloss: 4.159011  [28000/40538] loss: 0.355164  [28000/40538]\nloss: 4.145283  [29000/40538] loss: 0.358634  [29000/40538]\nloss: 4.370943  [30000/40538] loss: 0.378812  [30000/40538]\nloss: 4.190412  [31000/40538] loss: 0.374826  [31000/40538]\nloss: 4.072052  [32000/40538] loss: 0.374417  [32000/40538]\nloss: 4.098722  [33000/40538] loss: 0.361974  [33000/40538]\nloss: 4.312171  [34000/40538] loss: 0.363181  [34000/40538]\nloss: 4.137396  [35000/40538] loss: 0.367026  [35000/40538]\nloss: 4.322476  [36000/40538] loss: 0.375819  [36000/40538]\nloss: 3.957886  [37000/40538] loss: 0.353763  [37000/40538]\nloss: 3.953274  [38000/40538] loss: 0.358300  [38000/40538]\nloss: 4.142745  [39000/40538] loss: 0.365480  [39000/40538]\nloss: 4.269638  [40000/40538] loss: 0.365249  [40000/40538]\nEpoch 18\n-------------------------------\nloss: 4.172543  [    0/40538] loss: 0.362527  [    0/40538]\nloss: 4.263560  [ 1000/40538] loss: 0.361183  [ 1000/40538]\nloss: 3.953842  [ 2000/40538] loss: 0.356917  [ 2000/40538]\nloss: 4.200696  [ 3000/40538] loss: 0.367757  [ 3000/40538]\nloss: 4.097858  [ 4000/40538] loss: 0.365615  [ 4000/40538]\nloss: 3.768429  [ 5000/40538] loss: 0.354962  [ 5000/40538]\nloss: 4.168734  [ 6000/40538] loss: 0.357382  [ 6000/40538]\nloss: 4.264501  [ 7000/40538] loss: 0.376685  [ 7000/40538]\nloss: 4.200697  [ 8000/40538] loss: 0.359987  [ 8000/40538]\nloss: 4.026584  [ 9000/40538] loss: 0.368769  [ 9000/40538]\nloss: 4.188597  [10000/40538] loss: 0.381357  [10000/40538]\nloss: 4.167617  [11000/40538] loss: 0.364423  [11000/40538]\nloss: 4.479843  [12000/40538] loss: 0.371932  [12000/40538]\nloss: 4.123844  [13000/40538] loss: 0.360539  [13000/40538]\nloss: 3.751436  [14000/40538] loss: 0.345000  [14000/40538]\nloss: 4.148304  [15000/40538] loss: 0.365305  [15000/40538]\nloss: 4.129609  [16000/40538] loss: 0.360002  [16000/40538]\nloss: 4.055920  [17000/40538] loss: 0.366883  [17000/40538]\nloss: 3.832625  [18000/40538] loss: 0.360108  [18000/40538]\nloss: 4.017483  [19000/40538] loss: 0.361837  [19000/40538]\nloss: 4.019751  [20000/40538] loss: 0.359819  [20000/40538]\nloss: 4.226604  [21000/40538] loss: 0.366715  [21000/40538]\nloss: 4.115422  [22000/40538] loss: 0.361386  [22000/40538]\nloss: 4.174541  [23000/40538] loss: 0.372434  [23000/40538]\nloss: 4.275622  [24000/40538] loss: 0.376087  [24000/40538]\nloss: 3.994204  [25000/40538] loss: 0.356402  [25000/40538]\nloss: 4.319655  [26000/40538] loss: 0.369381  [26000/40538]\nloss: 4.162700  [27000/40538] loss: 0.368055  [27000/40538]\nloss: 4.146519  [28000/40538] loss: 0.362901  [28000/40538]\nloss: 4.144621  [29000/40538] loss: 0.373165  [29000/40538]\nloss: 4.309775  [30000/40538] loss: 0.376692  [30000/40538]\nloss: 4.240480  [31000/40538] loss: 0.373933  [31000/40538]\nloss: 4.155998  [32000/40538] loss: 0.361972  [32000/40538]\nloss: 4.177896  [33000/40538] loss: 0.365643  [33000/40538]\nloss: 4.007899  [34000/40538] loss: 0.359108  [34000/40538]\nloss: 4.044226  [35000/40538] loss: 0.372526  [35000/40538]\nloss: 3.955352  [36000/40538] loss: 0.361270  [36000/40538]\nloss: 4.252272  [37000/40538] loss: 0.359800  [37000/40538]\nloss: 4.017572  [38000/40538] loss: 0.369487  [38000/40538]\nloss: 4.113293  [39000/40538] loss: 0.364652  [39000/40538]\nloss: 4.252012  [40000/40538] loss: 0.377016  [40000/40538]\nEpoch 19\n-------------------------------\nloss: 4.113704  [    0/40538] loss: 0.360071  [    0/40538]\nloss: 3.883387  [ 1000/40538] loss: 0.356927  [ 1000/40538]\nloss: 4.118389  [ 2000/40538] loss: 0.369580  [ 2000/40538]\nloss: 4.008396  [ 3000/40538] loss: 0.367291  [ 3000/40538]\nloss: 4.167516  [ 4000/40538] loss: 0.360161  [ 4000/40538]\nloss: 4.041998  [ 5000/40538] loss: 0.360533  [ 5000/40538]\nloss: 3.940003  [ 6000/40538] loss: 0.357705  [ 6000/40538]\nloss: 4.085411  [ 7000/40538] loss: 0.360970  [ 7000/40538]\nloss: 4.386557  [ 8000/40538] loss: 0.363533  [ 8000/40538]\nloss: 3.952586  [ 9000/40538] loss: 0.358729  [ 9000/40538]\nloss: 4.214913  [10000/40538] loss: 0.358721  [10000/40538]\nloss: 4.097003  [11000/40538] loss: 0.358955  [11000/40538]\nloss: 4.003417  [12000/40538] loss: 0.372072  [12000/40538]\nloss: 4.378111  [13000/40538] loss: 0.376018  [13000/40538]\nloss: 4.296719  [14000/40538] loss: 0.375627  [14000/40538]\nloss: 3.937304  [15000/40538] loss: 0.352799  [15000/40538]\nloss: 4.178517  [16000/40538] loss: 0.376179  [16000/40538]\nloss: 4.077796  [17000/40538] loss: 0.360508  [17000/40538]\nloss: 4.153522  [18000/40538] loss: 0.357712  [18000/40538]\nloss: 3.986604  [19000/40538] loss: 0.361320  [19000/40538]\nloss: 4.105944  [20000/40538] loss: 0.361273  [20000/40538]\nloss: 4.382588  [21000/40538] loss: 0.373603  [21000/40538]\nloss: 3.984592  [22000/40538] loss: 0.358140  [22000/40538]\nloss: 4.216249  [23000/40538] loss: 0.377962  [23000/40538]\nloss: 4.032379  [24000/40538] loss: 0.355597  [24000/40538]\nloss: 4.109952  [25000/40538] loss: 0.363613  [25000/40538]\nloss: 3.986585  [26000/40538] loss: 0.360651  [26000/40538]\nloss: 3.978273  [27000/40538] loss: 0.352952  [27000/40538]\nloss: 4.225423  [28000/40538] loss: 0.365071  [28000/40538]\nloss: 4.236368  [29000/40538] loss: 0.359101  [29000/40538]\nloss: 4.076289  [30000/40538] loss: 0.365535  [30000/40538]\nloss: 4.000926  [31000/40538] loss: 0.363646  [31000/40538]\nloss: 4.065161  [32000/40538] loss: 0.367340  [32000/40538]\nloss: 4.075935  [33000/40538] loss: 0.363534  [33000/40538]\nloss: 4.064492  [34000/40538] loss: 0.356622  [34000/40538]\nloss: 4.273123  [35000/40538] loss: 0.366022  [35000/40538]\nloss: 4.137391  [36000/40538] loss: 0.367637  [36000/40538]\nloss: 4.318799  [37000/40538] loss: 0.373100  [37000/40538]\nloss: 4.222788  [38000/40538] loss: 0.370175  [38000/40538]\nloss: 4.166167  [39000/40538] loss: 0.362998  [39000/40538]\nloss: 4.164732  [40000/40538] loss: 0.363737  [40000/40538]\nEpoch 20\n-------------------------------\nloss: 3.975719  [    0/40538] loss: 0.355143  [    0/40538]\nloss: 4.060712  [ 1000/40538] loss: 0.372247  [ 1000/40538]\nloss: 4.368295  [ 2000/40538] loss: 0.377967  [ 2000/40538]\nloss: 3.934213  [ 3000/40538] loss: 0.358519  [ 3000/40538]\nloss: 3.791705  [ 4000/40538] loss: 0.367924  [ 4000/40538]\nloss: 3.984057  [ 5000/40538] loss: 0.354387  [ 5000/40538]\nloss: 4.302698  [ 6000/40538] loss: 0.375871  [ 6000/40538]\nloss: 4.066349  [ 7000/40538] loss: 0.363499  [ 7000/40538]\nloss: 4.011107  [ 8000/40538] loss: 0.355550  [ 8000/40538]\nloss: 4.069483  [ 9000/40538] loss: 0.363411  [ 9000/40538]\nloss: 3.962096  [10000/40538] loss: 0.360324  [10000/40538]\nloss: 3.879223  [11000/40538] loss: 0.359925  [11000/40538]\nloss: 4.139554  [12000/40538] loss: 0.366473  [12000/40538]\nloss: 4.238248  [13000/40538] loss: 0.369460  [13000/40538]\nloss: 4.325219  [14000/40538] loss: 0.373961  [14000/40538]\nloss: 4.046096  [15000/40538] loss: 0.364038  [15000/40538]\nloss: 3.973631  [16000/40538] loss: 0.364119  [16000/40538]\nloss: 4.130948  [17000/40538] loss: 0.371572  [17000/40538]\nloss: 4.110705  [18000/40538] loss: 0.367881  [18000/40538]\nloss: 3.981675  [19000/40538] loss: 0.358867  [19000/40538]\nloss: 3.969772  [20000/40538] loss: 0.367642  [20000/40538]\nloss: 4.213230  [21000/40538] loss: 0.369239  [21000/40538]\nloss: 4.396805  [22000/40538] loss: 0.372665  [22000/40538]\nloss: 4.135526  [23000/40538] loss: 0.358581  [23000/40538]\nloss: 4.274687  [24000/40538] loss: 0.359747  [24000/40538]\nloss: 4.018264  [25000/40538] loss: 0.362249  [25000/40538]\nloss: 4.215373  [26000/40538] loss: 0.362426  [26000/40538]\nloss: 4.146495  [27000/40538] loss: 0.360572  [27000/40538]\nloss: 4.178113  [28000/40538] loss: 0.365967  [28000/40538]\nloss: 4.127393  [29000/40538] loss: 0.367868  [29000/40538]\nloss: 4.328934  [30000/40538] loss: 0.371098  [30000/40538]\nloss: 4.118513  [31000/40538] loss: 0.361431  [31000/40538]\nloss: 4.275883  [32000/40538] loss: 0.366288  [32000/40538]\nloss: 4.156716  [33000/40538] loss: 0.365992  [33000/40538]\nloss: 4.060382  [34000/40538] loss: 0.364325  [34000/40538]\nloss: 3.891718  [35000/40538] loss: 0.356216  [35000/40538]\nloss: 4.203720  [36000/40538] loss: 0.362157  [36000/40538]\nloss: 4.303934  [37000/40538] loss: 0.373115  [37000/40538]\nloss: 4.109009  [38000/40538] loss: 0.371261  [38000/40538]\nloss: 4.147849  [39000/40538] loss: 0.363511  [39000/40538]\nloss: 4.025926  [40000/40538] loss: 0.361732  [40000/40538]\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Giá như hôm đó không tỏ tình thì có lẽ bầu trời vẫn còn xanh\"\n(char_input, word_input, mask_pos, target) = data_builder.build_data([text])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T03:23:42.145337Z","iopub.execute_input":"2024-07-24T03:23:42.145713Z","iopub.status.idle":"2024-07-24T03:23:42.152666Z","shell.execute_reply.started":"2024-07-24T03:23:42.145677Z","shell.execute_reply":"2024-07-24T03:23:42.151939Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"rere = model_fill(char_input.to(device0), word_input.to(device0), mask_pos.to(device0))","metadata":{"execution":{"iopub.status.busy":"2024-07-24T03:24:19.805431Z","iopub.execute_input":"2024-07-24T03:24:19.806108Z","iopub.status.idle":"2024-07-24T03:24:19.838504Z","shell.execute_reply.started":"2024-07-24T03:24:19.806076Z","shell.execute_reply":"2024-07-24T03:24:19.837590Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"target","metadata":{"execution":{"iopub.status.busy":"2024-07-24T03:24:50.800433Z","iopub.execute_input":"2024-07-24T03:24:50.800806Z","iopub.status.idle":"2024-07-24T03:24:50.807102Z","shell.execute_reply.started":"2024-07-24T03:24:50.800776Z","shell.execute_reply":"2024-07-24T03:24:50.806276Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[[5160, 3247],\n [1280, 8065],\n [7220, 704],\n [5370, 2485],\n [7301, 3028],\n [4229, 2102],\n [7095, 530],\n [7095, 530]]"},"metadata":{}}]},{"cell_type":"code","source":"rere.argmax(-1)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T03:24:38.864648Z","iopub.execute_input":"2024-07-24T03:24:38.864989Z","iopub.status.idle":"2024-07-24T03:24:38.872260Z","shell.execute_reply.started":"2024-07-24T03:24:38.864961Z","shell.execute_reply":"2024-07-24T03:24:38.871292Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([5160, 4808, 1280, 8065, 7220,  704, 5370, 2485, 7301, 2771, 4229, 2102,\n        7095,  338, 7095,  530], device='cuda:0')"},"metadata":{}}]}]}